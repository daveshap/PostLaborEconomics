Cognitive Capture in the Post-Labor Age The Post-Labor Economy: Abundance, Automation, and the End of Work Advances in artificial intelligence and automation are driving humanity toward a "post-labor" economic paradigm. In a post-labor economy, human labor is no longer the central engine of production or source of income . Scholars describe this as a fundamental shift beyond the traditional wage-based system: machines and algorithms increasingly perform work, potentially eliminating whole categories of jobs rather than merely transforming them . Such an economy promises unprecedented material abundance. Technologies like advanced robotics, AI, and renewable energy are sharply reducing marginal costs, leading some to envisage a near post-scarcity society . As journalist Paul Mason and economist Jeremy Rifkin have argued, the convergence of digital and automated technologies drives the marginal cost of many goods and services toward zero, weakening the logic of market scarcity on which industrial capitalism was built . In theory, if human labor and resource costs diminish dramatically, production could meet all basic needs with little scarcity, fundamentally altering economic relationships . This optimistic vision of abundance must be tempered by practical constraints. Even if labor becomes effectively unlimited through automation, other scarcities persist – finite natural resources, energy limits, and environmental constraints . Some economists urge caution with the term "post-scarcity," suggesting that an automated future might still require conscious management of resources and ecological limits . For instance, proponents of degrowth like Kallis (2018) argue that a fully automated economy should focus on sufficiency and wellbeing rather than endless growth . In this view, automation’s productivity gains could allow comfortable living standards for all within planetary boundaries, instead of fueling unbounded consumption . Additionally, certain domains may remain scarce or even grow in value – authentic human experiences, creativity, and craftsmanship may become premium commodities when mass-produced goods are abundant . Thus, the post-labor future is complex: mundane products might be cheap and plentiful, but human-centric services or cultural goods could command high value as luxury items in an automated world . A critical question in the post-labor transition is the collapse of the wage relation and the distribution of wealth . If wages – earned through labor – cease to be the primary vehicle for distributing purchasing power , new mechanisms must emerge to ensure people can meet their needs and share in the prosperity generated by automation . One prominent proposal is the Universal Basic Income (UBI) : a regular , unconditional cash payment to every individual. UBI would decouple income from employment , providing an economic floor in a world with precarious or vanishing jobs . Proponents cast UBI as a social dividend of automation , a way to distribute the fruits of robotic productivity to all citizens . Notably, experiments have shown UBI can improve well-being without destroying work incentives – for example, Finland’s 2017–2018 basic income trial improved health and stress outcomes for recipients without causing them to exit the labor force . Critics, however , question whether UBI alone can address deeper inequalities of wealth and power; a modest basic income might alleviate poverty but leave existing capital concentrations untouched . Some argue for complementary or alternative models like a federal Job Guarantee – directly providing public employment to those who want it – to preserve the social benefits of12 12 34 35 64 78 79 1011 1213 1415 1617 1819 2021 2223 2425 2627 1 work in an era where its necessity is diminished . The debate underscores that the end of work challenges not only our economic structures, but also cultural values attached to labor , purpose, and distributional justice. Equally important is who owns and controls the means of automated production . If AI and robotics become the primary productive forces, outcomes will diverge dramatically depending on ownership patterns . One scenario is concentrated private ownership : a small group of investors and tech firms controlling the AI and robot infrastructure. In that case, profits and economic gains would accrue almost entirely to them, exacerbating inequality and potentially creating a new neo-feudal divide between a wealthy owner class and a disenfranchised majority . Without intervention, economists warn this could lead to extreme concentration of wealth; as Korinek and Stiglitz (2018) explore, capital owners might reap all returns from automation while others are left with little, a dynamic reminiscent of elite capture of capital . Piketty’s work likewise suggests that without progressive taxation or redistribution, returns on capital will outpace broad growth, entrenching inequality . Alternatively, models of public or shared ownership could spread the benefits. Proposals range from publicly owned AI utilities that pay social dividends, as envisioned by Yanis Varoufakis (2021), to cooperative and community ownership of platforms and robots . Historical precedents like Alaska’s oil dividend – where resource revenues are shared with all residents – are cited as analogies for how automated wealth might be broadly distributed if the underlying assets are treated as a public commons . In sum, the post-labor economy poses profound questions: not just how we produce in an age of abundance, but how we ensure that abundance yields human flourishing rather than oligarchy. Economic theory is beginning to grapple with these issues across multiple dimensions – technological, ethical, and political – setting the stage for new societal compacts in the absence of traditional work . From Elite Capture to Cognitive Capture: Epistemic Infrastructure as the New Power Nexus The shift to a post-labor economy is accompanied by a subtler shift in the locus of power: from the ownership of capital to the ownership of attention and knowledge . In classical terms, elite capture occurs when a small group seizes control of resources or institutions meant for the broader public, bending them to serve its own interests. Historically, elites captured economic capital (land, industrial assets, financial systems) and political institutions, leveraging their position to perpetuate advantage. In the 21st century, however , a new form of capture is emerging: cognitive capture , the domination of epistemic and attentional infrastructure by powerful actors. Unlike traditional elite capture of material capital, cognitive capture is about controlling the channels of information, perception, and belief – the infrastructure of knowledge itself. Cognitive capture can be seen as an outgrowth of what regulatory scholars have called cultural capture or cognitive capture in governance: when regulators and experts unconsciously adopt the worldview of the industries or elites they oversee, not through bribes but through belief-shaping . In one formulation, “regulators may come to view the world the way firms do, not because they have been captured through incentives, but because they have been convinced” . This concept has now scaled beyond regulatory agencies. In our networked society, entire populations risk being cognitively captured by those who control digital platforms, media ecosystems, and AI-driven recommendation systems. The architects of our information environment – tech giants, state surveillance apparatuses, and media conglomerates – wield unprecedented influence over public attention and perception. By shaping what information people see (or2829 3031 3233 3234 3536 3738 3938 40 2 4142 4142 2 do not see), these actors capture the epistemic infrastructure (the means by which people acquire knowledge) and the attentional infrastructure (the means by which people’s attention is directed and sustained). The mechanisms of cognitive capture are both pervasive and insidious. Consider the landscape of social media and search engines: a few dominant platforms serve as gatekeepers of information for billions. Through personalized algorithms tuned for engagement, these systems subtly curate reality for each user , selecting which news, opinions, or products appear before us. This creates fertile ground for what Shoshana Zuboff terms “instrumentarian power” – a mode of control that works not by force, but by instrumenting and tuning human behavior via surveillance data and algorithmic feedback . As Zuboff observed, big tech platforms deploy vast troves of personal data and psychological insights to nudge and coax users toward desired behaviors (be it clicking an ad or adopting a political viewpoint) . Coupled with AI, these techniques amount to a project of total behavioral domination, a Big Brother not of the state but of the marketplace – what Zuboff calls the Big Other . In her analysis of surveillance capitalism, once machine learning algorithms are paired with behavioral data, companies can “realize a project of total domination over society” by automating the manipulation of human choices . Such power seizes the attentional commons – our collective capacity to focus – and repurposes it for profit and social control. This cognitive capture is elite capture by other means . Instead of seizing factories or legislatures, today’s elites (whether corporate executives, autocratic governments, or even influential cabals of online actors) seize the levers of human attention and understanding. They capture the minds of citizenry in two reinforcing ways: Control of Information (Epistemic Capture): By dominating news outlets, social media feeds, search algorithms, and educational content, elites can skew the population’s perceived reality. They can bury certain truths and amplify falsehoods or trivialities – achieving consent through propaganda or obfuscation . The flow of expert knowledge can likewise be hijacked. For instance, industries have been known to create epistemic communities of hired experts and front groups to cast doubt on inconvenient science (the “merchants of doubt” strategy) . In the environmental and public health domains, corporations have colonized the space of scientific debate by funding sympathetic research and advocacy, effectively capturing the very standards of evidence and reason . The result is a populace (and often regulators as well) who internalize the elite’s perspective without realizing it. When the channels of knowledge – universities, journals, media – are subtly co-opted, society experiences epistemic capture , losing the capacity to critique power on an informed basis. Control of Attention (Attentional Capture): Modern digital platforms operate on the attention economy , treating human attention as the scarce commodity to be monetized . Through constant notifications, endless scrolling feeds, personalized recommendations, and even addictive game-like design, these platforms maximize the time users spend engaged . The result is that billions of people’s daily attention is directed and partitioned largely by a handful of tech companies. This attentional capture can distract the public from important issues (fostering apathy or triviality as in Huxley’s dystopia) and also serve as a channel to push messaging favorable to those in control. As Nobel Laureate Herbert Simon presciently noted, “a wealth of information creates a poverty of attention” . By flooding every waking moment with algorithmically curated content, elites can crowd out reflection, dissent, or alternative sources of meaning. In a literal sense, they capture the neurological real estate of our brains – our time and cognitive bandwidth.43 43 43 43 • 4445 4647 • 48 4950 50 3 Cognitive capture is not a distant theoretical concern; it pervades everyday life in the digital age. When a social media platform’s algorithm quietly decides which political posts you see, it may be capturing your political agency . When YouTube’s autoplay nudges someone down an extremist rabbit hole or a QAnon conspiracy, that is cognitive capture at work through attentional hijacking. When a search engine autofill shapes what questions you even think to ask, it has captured a piece of your epistemic autonomy. Over time, these small manipulations aggregate into a formidable form of social control – one that operates by structuring the field of what is cognitively accessible, thinkable, or desirable to the public. Importantly, cognitive capture today merges Orwellian and Huxleyan methods (coercion and distraction), as we will explore in detail. It is not only about persuading people of a certain worldview, but also about keeping them too entertained, anxious, or polarized to question the structures of power . The concept goes beyond mere “filter bubbles” or “echo chambers”; it suggests an entire political economy where knowledge itself is enclosed . Just as, in earlier eras, elites enclosed the commons or controlled capital, now they seek to enclose the cognitive commons – the shared space of reasons and attention on which democratic deliberation relies. Cognitive capture is thus a modern evolution of elite domination, weaponizing the insights of behavioral science, AI, and network effects to consolidate power over minds rather than solely over markets . In summary, elite capture has shifted from a primarily economic phenomenon to a cognitive one . Ownership of the means of production still matters, but ownership of the means of information production and absorption may matter even more in a post-labor , digitally intermediated world. The ramifications are profound. If a small elite controls the master algorithm that billions use to get news, or the only search index through which knowledge is found, or the most addictive sources of entertainment and social validation – then that elite can shape reality itself for society. Such power calls for new frameworks of accountability and resistance, as the traditional checks (competition, regulation, an informed public) struggle to keep up. The next sections delve into how this cognitive capture operates along two classic dystopian models – Orwell’s and Huxley’s – and how AI-driven surveillance technologies intensify this dynamic. Orwell vs. Huxley: Coercive Surveillance and Distractive Seduction The struggle for control over minds in the digital era can be understood through two archetypal paradigms of dystopian control: the Orwellian and the Huxleyan. George Orwell’s Nineteen Eighty-Four (1949) painted a grim picture of a society ruled by fear , coercion, and omnipresent surveillance – a world in which truth is whatever the regime declares, and dissent is crushed via torture and propaganda. Aldous Huxley’s Brave New World (1932), by contrast, envisioned a society controlled through pleasure, constant entertainment, and psychological conditioning – a populace kept docile not by terror , but by satisfaction of shallow desires. These two visions have long been treated as opposing warnings; media theorist Neil Postman famously contrasted them: “Orwell feared those who would ban books; Huxley feared no one would want to read one. Orwell feared the truth would be concealed from us; Huxley feared it would be drowned in a sea of irrelevance… In Orwell’s prophecy, people are controlled by inflicting pain; in Huxley’s, by inflicting pleasure.” . Today, elements of both dystopias are visible, often intertwined, as dual vectors of cognitive capture . Orwellian surveillance represents the coercive vector: the use of monitoring, fear , and repression to enforce conformity. In 1984 , Big Brother’s telescreens watch citizens relentlessly, and any deviation in thought (thoughtcrime) invites swift punishment. This paradigm is echoed in modern forms of authoritarian tech-powered surveillance. Governments with Orwellian tendencies deploy AI-enhanced cameras, facial4142 51 4 recognition, and data-mining of communications to achieve near-total visibility into citizens’ lives. The goal is to instill a chilling effect – if you know you might be watched at any moment, you police yourself. Contemporary examples include China’s extensive surveillance state, where cameras and algorithms track individuals to maintain “social stability,” or the revelations of mass electronic spying by agencies like the NSA. The logic of fear remains: people can be controlled by the threat of punitive observation. Yet unlike Orwell’s world of scarcity and terror , much of today’s surveillance is couched in terms of security and convenience. As one commentator noted, Orwell’s dystopia was one of a boot stamping on a human face, a society of open oppression and hardship . It’s fair to ask: could such a purely fear-based system truly endure in the long run? Huxley himself doubted it – in a 1949 letter to Orwell, Huxley predicted that “infant conditioning and narco-hypnosis [would be] more efficient instruments of government than clubs and prisons,” and that rulers would “learn that a thoroughly pleased populace is easier to control than a terrified one.” . Modern authoritarian regimes seem to take this to heart by blending hard surveillance with paternalistic promises of order and even consumer comforts. For example, China’s nascent “social credit” system pairs surveillance data with gamified rewards and punishments, nudging citizens toward approved behavior with carrots as well as sticks. This begins to bridge to the Huxleyan paradigm . Huxleyan surveillance – if we can call it surveillance at all – is the distractive, seductive vector of cognitive capture. In Brave New World , people are bred and conditioned to love their servitude. They are endlessly entertained by trivial diversions, pacified by the drug soma , and numbed by a flood of instant gratification. There is no need for an omnipresent Big Brother because the populace, placated and narcotized by pleasure , pose no threat of rebellion. Huxley’s World State controls people not by watching and scaring them, but by satisfying their shallow desires so completely that higher aspirations (truth-seeking, liberty, critical thought) wither away. The motto could be: why censor books when no one wants to read? This vision resonates uncannily with aspects of the digital consumer society. We live in a world of infinite content, streaming entertainment, social media dopamine loops, and personalized ads – a “sea of irrelevance” that can submerge truths and depth in favor of memes and trivia . In modern democracies, power often perpetuates itself not by open coercion but by cultivating complacency and distraction. The average citizen is bombarded with notifications, videos, and shopping recommendations tailored to their preferences, keeping them continuously busy consuming or scrolling. In such an environment, the question “are we free?” rarely arises – as Postman quipped, we have come to adore the technologies that undo our capacities to think . Huxley’s dictators discovered “man’s almost infinite appetite for distractions,” using it as the ultimate means of pacification . This is visible today in how algorithm-driven feeds keep individuals hooked, servitude sugar-coated as entertainment. People love their servitude when it looks like a personalized news feed and same-day delivery of consumer goods. Crucially, the Orwellian and Huxleyan methods are not mutually exclusive – they converge in our era of AI and big data . A 2024 analysis put it aptly: “In many ways, 1984 and Brave New World are merging into a single dystopia.” Modern society exhibits Orwellian elements like mass surveillance and data harvesting akin to telescreens, yet these are often presented as useful conveniences (e.g. location tracking for personalized services) rather than explicit tyrannical tools . Simultaneously, we see Huxleyan elements in the addictive nature of social media, endless streams of content, and consumerist saturation, which fulfill Huxley’s warning that the most effective control comes from manipulating desires rather than ruling by fear . The result is a dual system: coercion where necessary, distraction everywhere else .52 53 5455 56 5758 59 60 6162 5 For example, consider the smartphone – a single device that exemplifies this merger . It is Orwell’s telescreen in that it can track your movements, listen through the microphone, log your messages, and feed data to state or corporate monitors. But it is also Huxley’s soma in that it delivers a constant stream of entertainment, social validation, and information tidbits that can be as habit-forming as any drug. With one gadget, authorities (or companies) gain unprecedented surveillance capabilities, and users gain an endless source of distraction that they willingly keep by their side at all times. The psychological effect is a blend of self-policing and self-numbing . People may restrain certain behaviors or speech because they sense the digital panopticon’s gaze (even if just the gaze of social media peers), while also voluntarily spending hours in virtual amusements that divert energy from civic engagement or deep reflection. In the political realm, fear and pleasure cooperate to produce compliance . A populace that is monitored is less likely to organize overt dissent (Orwell’s effect), and a populace that is entertained and gratified is less likely to feel dissent is necessary (Huxley’s effect). As one analysis noted, “modern democracies reflect Huxley’s vision, where consumerism, entertainment, and technology keep the masses distracted,” whereas “Orwell’s world finds echoes in authoritarian states where fear is the primary tool of control” . But most tellingly, our world shows a blend of both : mass surveillance (like Orwell) is often framed as or hidden within pleasurable services (like Huxley) . For instance, targeted advertising is a form of propaganda and thought-shaping, but it arrives as amusing or enticing personalized content – a Huxleyan veneer over an Orwellian practice. Social media platforms may quietly suppress or promote certain political content (an exercise of power over truth akin to the Ministry of Truth) yet do so through algorithmic feeds that users voluntarily scroll for enjoyment. This duality can be observed in concrete scenarios. The emergence of “smart city” surveillance – cameras, sensors, AI analytics in urban environments – is often justified by security (Orwellian logic) but is also tied into conveniences like smart traffic management or personalized city apps (Huxleyan appeal). The growth of “home assistant” AI devices (Amazon Echo, Google Home) similarly brings microphones into every home (a potential Orwellian intrusion) under the friendly guise of a helpful, entertaining assistant ready to play music or answer trivia (the Huxleyan lure). Even the realm of workplace or school surveillance – AI tools to monitor productivity or proctor exams – combines the threat of oversight with gamification or “productivity rewards” that make surveillance feel participatory. The melding of these paradigms has been succinctly described as the iron fist inside the velvet glove . As a commentary on Orwell vs. Huxley observed, “Orwell reminds us to beware the iron fist, while Huxley warns us of the velvet glove” . In our context, the iron fist is the data-driven surveillance state/corporation that can exert punitive power , and the velvet glove is the seductive digital ecosystem that keeps us docile. A populace subject to both will rarely recognize the chains that bind it: if you do, fear holds you back from speaking out; if you don’t, it’s likely because you’re too pleasantly distracted to notice the need. In summary, Orwellian and Huxleyan paradigms function as dual vectors of cognitive capture . The Orwellian vector uses surveillance, data policing, and sometimes overt coercion to constrain behavior through fear . The Huxleyan vector uses entertainment, consumer rewards, and information overload to constrain thought through pleasure . Together they create a society of compliant citizen-subjects who self- censor and self-soothe . In the next section, we examine how AI systems supercharge both vectors – acting simultaneously as monitoring tools and as instruments of mass pacification.6364 60 6566 6 AI as Panopticon and Pacifier: Fear and Pleasure in Algorithmic Governance Artificial intelligence has rapidly become a dual-purpose instrument of social control, serving as both panopticon and pacifier . In its panopticon role, AI-driven surveillance monitors and disciplines populations (the Orwellian impulse). In its pacifier role, AI-driven media and personalization systems seduce and satisfy populations (the Huxleyan impulse). The blending of these functions allows modern governance – whether by state or corporate entities – to engineer compliance by mixing fear and pleasure , creating subjects who are both watched and coddled. On the surveillance side, AI dramatically extends the reach of the Panopticon (Jeremy Bentham’s design later analyzed by Foucault as a metaphor for modern disciplinary power). We now face a “digital panopticon” in which ubiquitous sensors and interconnected databases enable continuous observation without the need for a physical tower . Machine learning algorithms can analyze vast streams of data – CCTV feeds, geolocation pings, social media posts, financial transactions – to flag “unusual” behavior or identify individuals. This goes far beyond human-scale watching; it is automated suspicion at scale. As one author imagined, if Foucault were alive today, he might see AI as an unprecedented mode of surveillance and control that far exceeds the capabilities of human watchers . AI can process and learn from enormous datasets, spotting patterns a human would miss, thereby acting as an all-seeing eye that penetrates every corner of digital interaction . Examples abound. Facial recognition algorithms can pick a face out of a crowd of thousands in real time, allowing authorities to track an individual’s movements across city cameras or identify protest participants. Predictive policing models analyze crime data to forecast where illegal activity might occur or who is likely to be involved, effectively subjecting certain neighborhoods or persons to heightened pre-emptive scrutiny. Internet surveillance AI can sift through emails or messages for keywords or patterns indicating subversive intent. In combination, these technologies create an environment akin to Orwell’s Thought Police , though more subtle: one may not even realize when or how one is being “seen” by the algorithm. The result is a society in which, potentially, “one cannot know when one is being watched and is therefore compelled to self- regulate at all times” . As Foucault taught, this internalization of surveillance is the key to panoptic control – people discipline themselves under the possibility of an observing gaze. AI makes that gaze effectively omnipresent and hyper-efficient, eroding any remaining pockets of unmonitored life. The erosion of privacy by AI surveillance tilts the power dynamics toward the watchers, whether they be authoritarian governments or unaccountable tech companies . The blurring of lines between watcher and watched also occurs, as we voluntarily carry devices that report on us and even partake in lateral surveillance (e.g. recording each other , sharing data) . It is a participatory panopticon : we are coerced into participating in our own surveillance through reliance on digital systems for everyday life . While these AI systems instill a background fear (the knowledge that straying from the norm might trigger an algorithmic red flag), AI simultaneously operates as a pacification and persuasion apparatus . Consider the algorithms curating your news feed or recommending your next video. They leverage AI to predict what will hold your attention or appeal to your preferences, thereby delivering continuous micro-doses of pleasure, validation, or interest. This is the Huxleyan face of AI – the algorithm as digital soma dispenser . A personalized feed, tuned by reinforcement learning, becomes “a continually updating stream” of content tailored to your tastes . This provides immediate gratification and creates a bubble of comfort: one6768 6970 7071 6768 7273 7374 73 74 7576 7 sees largely what one likes to see, hears what one likes to hear . In such an echo chamber , challenges to one’s worldview are minimized, leaving individuals more complacent and confident in their beliefs (even if those beliefs are misguided). Thus, AI-driven curation can entrench intellectual passivity and insulate people from discomforting truths – a recipe for triviality and acquiescence. Moreover , AI-powered platforms strive to maximize “engagement,” which often means exploiting psychological vulnerabilities to keep users hooked. The use of A/B testing , adaptive algorithms, and behavioral data means the system learns in real time how to better entice each user – whether through outrage, sensationalism, or flattery. Much like Skinner’s operant conditioning experiments, the algorithm dispenses rewards (a surge of social notifications, a juicy piece of gossip, a funny meme) to reinforce behaviors (keep scrolling, keep clicking) that serve the system’s goals. In effect, AI algorithms function as millions of customized Skinner boxes , training each of us to engage in ways that generate profit or influence. This creates a passive citizenry not because of sedation by chemicals, but sedation by algorithmic feedback loops and the sheer ambient noise of digital life. People become accustomed to having their emotional needs immediately met by technology – whether it’s the need for distraction when bored or the need for affirmation when lonely. Over time, this fosters a dependency and docility : as Marshall McLuhan observed through the myth of Narcissus, one who becomes enamored of an extension of themselves (like personalized media reflecting their preferences) can become numb to reality, “hypnotized by the amputation and extension of his own being” . We are all Narcissus now , staring into the pool of social media and not recognizing it is our own reflections we see, numb to the manipulation underlying it . The combination of these phenomena – AI panopticism and AI pacification – yields what might be called the compliant subject . Surveillance ensures that outright deviance is risky; algorithmic gratification ensures there is seldom a felt need to deviate in the first place. Sociologists might see here a new mode of governance: neither pure discipline (as in Foucault’s factories and prisons) nor pure spectacle, but an integrated system of governance through comfort . By “blending fear and pleasure” , AI-enabled systems manage to calibrate behavior finely. For instance, on social media platforms, users quickly learn the boundaries of acceptable discourse – not necessarily through explicit warnings, but because the algorithms quietly demote posts that are flagged or unpopular , while rewarding conformist or sensational takes with more visibility. The user , often unconsciously, adapts to what “works” for gaining attention (pleasure) and avoids what leads to isolation or reprimand (fear). In this manner , social norms are engineered by the platform in alignment with its and its sponsors’ interests. What emerges is a form of soft totalitarianism : not a jackbooted police state, but a gamified, data-driven order where people cheerfully police themselves and each other . Even in overtly authoritarian contexts, regimes are recognizing the value of mixing the two approaches. As noted earlier , China’s authorities use extensive surveillance (CCTV, facial recognition, internet monitoring) to enforce rules (Orwellian), but they also use social credit incentives, propaganda entertainment, and nationalist consumer culture to keep people content (Huxleyan). In democracies, corporate power often plays the primary role: Big Tech companies surveil user behavior extensively (for monetization, which can be co-opted by government via subpoenas or secret requests) and simultaneously flood the zone with conveniences and content that users find irresistible. Thus the line between Big Brother and Big Tech’s “Big Other” blurs. We voluntarily yield data (where we go, what we buy, who we know) because the services provide useful or fun functionalities – we trade privacy for convenience, freedom for entertainment. A useful symbolic juxtaposition might be the Camera and the Candy . The camera (or sensor) instills restraint: knowing one is observable, one is less likely to take certain risky actions. The candy – digital candy7778 7778 8 in the form of streams of content and consumer delights – keeps one satisfied and distracts from asking “why so many cameras?” Together , they form a carrot-and-stick approach automated by AI. The stick is often invisible (one might never know one was denied an opportunity because an algorithmic background check flagged some behavior), and the carrot is omnipresent (the next show auto-plays, the next product is one-click away). This creates a psychic numbing . Some philosophers like Byung-Chul Han have argued we live in a “transparent society” where we exploit our own freedom, constantly sharing and consuming in ways that make external domination almost unnecessary – we volunteer into servitude by pursuing digital pleasure , while subtle systems guide our choices. To be sure, this portrayal should not be understood as implying total success of cognitive capture. Human beings still have agency, subversion still happens, and awareness of these dynamics is growing. Yet the trend is clear: the symbolic architecture of control has shifted from the visible, industrial-era chains to the invisible, informational-era webs . The next section will explore the deeper archetypes and narratives that underlie this shift, showing how ancient symbols like Prometheus and Narcissus, and modern parables like Skinner’s Walden Two , can illuminate the psychological and cultural dimensions of our current predicament. Archetypes of Control: Prometheus, Narcissus, and Walden Two To fully grasp the cultural and psychological underpinnings of cognitive capture, it helps to cast our eyes to myth and metaphor . Myths and narratives provide a symbolic vocabulary for understanding how humans relate to technology and power . In the context of post-labor surveillance society, three figures stand out: Prometheus , Narcissus , and the utopian community of Walden Two (from B.F. Skinner’s novel). Each offers a model or cautionary tale for aspects of control – Prometheus for the gift and curse of technology, Narcissus for self-obsession and numbness, and Walden Two for engineered happiness that may mask the loss of freedom. Prometheus , in Greek mythology, is the titan who stole fire from the gods and gave it to humanity, enabling civilization and technical prowess . Fire in this myth symbolizes knowledge, technology, and power – essentially the tools of progress. By giving fire to humans, Prometheus empowered mankind but also incurred the wrath of Zeus. His punishment was eternal: bound to a rock while an eagle devoured his regenerating liver each day . The story of Prometheus speaks to the double-edged nature of technological advancement. On one hand, technology (fire, and by extension AI or other modern “flames”) is a promethean gift that can liberate humanity from toil – the post-labor abundance we discussed is itself a Promethean aspiration . On the other hand, the myth warns of the cost of defying the gods , i.e., the unintended consequences or retributions that follow technological empowerment. In our modern narrative, one might say that humanity’s elites have become Prometheus unbound – wielding AI (a kind of divine fire) to remake the world, but without adequate foresight of consequences. Alternatively, one could see Prometheus in the masses : technology has been given to everyone (smartphones put immense power in ordinary hands), and now the “gods” – whether states or tech moguls – fear the empowered public and seek to chain or monitor them lest they overthrow the old order . In this analogy, surveillance and cognitive capture are Zeus’s eagles , punishing or containing the Promethean potential of a fully informed and technologically empowered populace. There is also a Promethean bargain evident: we have accepted the gifts of Big Tech’s fire (AI, internet, convenience) but perhaps underestimating the price – our privacy, our autonomy, our internal organs (metaphorically speaking) being feasted on by the data economy. The Promethean myth thus frames the symbolic architecture of control as a struggle over who wields the fire79 80 9 of advanced technology. Will it be an emancipatory force distributed to all, or will the “gods” of the modern age manage to keep it under their control and punish those who wield it freely? The figure of Narcissus , from Ovid’s Metamorphoses , offers a different angle – one focusing on self- infatuation and the paralysis it induces . Narcissus was a beautiful youth who fell in love with his own reflection in a pool, not realizing it was himself. He became so entranced by the image that he could not leave it, eventually wasting away, transformed into a flower at the water’s edge . Marshall McLuhan famously interpreted this as “Narcissus narcosis” : Narcissus was numbed (narco- from the Greek for numbness) by the technological extension of himself (the reflection, a mirror image) . He did not recognize the image as self and thus became a passive object of his own attention. McLuhan likened all media and gadgets to that mirror – extensions of ourselves that we become fascinated by and unconsciously self-identify with , leading to a kind of self-amputation of perception . We become the servants of our tools, idolizing them and in doing so, idolizing ourselves without knowing it . In today’s context, the Narcissus myth speaks volumes about social media and the attention economy . Through platforms like Facebook, Instagram, TikTok, we literally present reflections of ourselves to the world (carefully curated images and posts) and receive feedback in the form of likes and comments. It is easy to become entranced by this digital mirror – checking one’s own profile, seeking validation, tweaking one’s persona. Society at large stares into the collective pool of media, often losing sight of any distinction between genuine self and reflected image. The phrase “we are all Narcissus now” captures how ubiquitous smartphones and social networks have made us numb to the devices as extensions of ourselves . We carry these extensions (phones) as part of our being, and disconnecting from them feels like a death of identity (McLuhan noted that discarding our gadgets feels like “total suicide” because they have become part of us) . Narcissus symbolizes the Huxleyan side of cognitive capture – the idea that we voluntarily imprison attention in the superficial self, in endless self-reflection and self-consumption, rather than looking outward critically. It is a mythic illustration of how pleasure and vanity can overpower agency : Narcissus had the freedom to leave but did not, just as we have, in theory, the freedom to log off or resist trivial distractions, yet we often do not. The myth thus warns that self-love in the mirror can be the tool of our capture – in modern terms, the tailored algorithmic feeds and selfie cultures make us love the digital reflection, rendering us politically inert and cognitively captive. As long as we are insatiably gazing into screens , our own eyes “destroy” us (to quote the poem in Morrissey’s essay) , because they direct all our desire and energy into an illusion. Finally, Skinner’s Walden Two offers a 20th-century fable about engineered society that resonates uncannily with our algorithmic present. B.F. Skinner’s Walden Two (1948) is a fictional utopian community built on the principles of behavioral psychology – essentially a grand experiment in behavioral engineering for social harmony . In Walden Two, citizens live under gentle but pervasive conditioning. Free will is downplayed; instead, the environment is arranged to reinforce desirable behaviors and discourage undesirable ones, from early childhood onward . The community achieves high levels of order , productivity, and even a certain kind of happiness: work days are short, art and play are encouraged, conflict is minimized. But this comes at the cost of true autonomy – the inhabitants are, in effect, lab subjects who have been conditioned to find fulfillment in exactly the activities and roles the community needs them to fulfill . They “love their servitude,” to borrow Huxley’s phrase, because they have been scientifically guided to do so from birth. Skinner’s novel was controversial because it rejected the notion of freedom and soul; it posited that behavior is fully determined by environmental variables , and by systematically altering those variables, a near-8177 8277 7778 8384 7778 8586 8788 8990 9190 10 utopia could be achieved . In essence, Walden Two is a proto-algorithmic society – before digital computers, Skinner imagined the algorithm of behavioral psychology running a community. Fast forward to today: tech companies and governments are, in practice, implementing Walden Two-like ideas through AI algorithms . Instead of human Planners like Frazier in the novel, we have recommender systems and nudging architectures. These systems observe behavior (as Skinner observed his pigeons), analyze what stimuli produce what responses, and then alter the contingencies to shape future behavior . Consider how Facebook’s feed might promote posts that keep you engaged or Twitter’s algorithms might amplify outrage because it increases retweets. These are analogous to Skinner’s use of rewards (positive reinforcement) to condition behavior . Likewise, think of “gamified” educational apps or workplace productivity tools – they offer points, stars, or social recognition badges for completing tasks, essentially turning labor into a conditioned game. This is Walden Two’s philosophy in digital form: people will do what you want if you reinforce them properly . And it largely works – millions chase virtual rewards and adjust their actions to platform incentives daily. The Walden Two allegory forces us to confront the ethical cost of a comfortable, well-managed society . In the novel, the community is stable and content, but an outside philosopher visiting (Professor Castle) questions whether the people have truly chosen their lives or been deprived of choice and deeper meaning. Today we can ask similarly: if algorithms optimize our news, our entertainment, even our dating partners for maximum satisfaction, are we happier or just more managed? Skinner would argue the distinction is moot if people report being happy. But critics counter that without the freedom to err or to seek unprogrammed aspirations, we lose something essentially human. This debate is playing out in real life through the pushback against algorithmic decision-making: for example, students protest when algorithmic exam proctors invade their privacy, or consumers bristle at the idea that an AI might decide their creditworthiness or job prospects without transparency. It’s a desire to retain agency and dignity against an onslaught of Skinnerian optimization. Walden Two’s residents don’t riot or protest – they have no need to; their wants are satisfied in the way the community deems best. Similarly, a fully cognitively captured populace wouldn’t revolt – they would either be too pleased or too normalized to see the need. But is that peace, or is it the peace of the dystopian “well- adjusted” in a maladjusted society (to paraphrase Krishnamurti’s famous saying about not confusing health with the ability to function in a sick society)? Walden Two also notably limited work to 4 hours a day and encouraged arts – ironically parallel to post-labor promises – but it filled the freed time with a matrix of subtle controls to ensure people used it “constructively.” Today’s promise that automation will free leisure hours risks a similar caveat: if those hours are simply filled with more algorithmically curated consumption, the human spirit might not flourish as imagined. In sum, these archetypal references enrich our understanding of modern control: Prometheus : Symbolizes our relationship with transformative technology (AI as fire) – empowerment coupled with punishment or unintended consequences, and the struggle between sharing power widely vs. containing it. Narcissus : Embodies the danger of self-absorption facilitated by technology – the more we extend ourselves into the digital, the more we might fall in love with the extension and lose critical awareness, becoming numb and easily guided. Walden Two : Anticipates a society of engineered behavior – comfort and order achieved by surrendering freedom to a system that “knows best,” eerily presaging algorithmic governance and the ethical dilemmas it poses.9190 • • • 11 Each narrative highlights a different facet of the symbolic architecture of control shifting from the industrial to the informational . Where the industrial age gave us metaphors like chains, cages, and machines , the informational age gives us webs, mirrors, and labs . Yet the core questions remain ancient: Who gets to play god with the fire of knowledge? How easily do we fall for illusions of ourselves? Can happiness be genuine without freedom? These questions lead us to consider how our legal and constitutional frameworks are responding (or failing to respond) to this brave new world of cognitive capture. From Industrial Panopticon to Digital Matrix: The Shifting Architecture of Control The industrial era conceived of social control in largely mechanical and spatial terms : factories, prisons, schools, and hospitals were structured as enclosed spaces with strict routines, a paradigm Michel Foucault famously termed disciplinary society . In those “vast spaces of enclosure,” individuals moved from one institutional box to another (home to school to barracks to factory) and were molded by hierarchical surveillance and normalization within each . The emblem of this era was Bentham’s Panopticon – a prison design enabling a single guard to observe all inmates without them knowing when they’re watched. Foucault used it as a metaphor for how power makes itself invisible yet pervasive , inducing self-discipline in those aware they might be watched. This was a model of power suited to the 18th-19th centuries’ needs: to produce efficient workers, obedient soldiers, and docile bodies to operate the machinery of a capitalist, nation-state system . However , as Foucault noted and Gilles Deleuze later expanded upon, the 20th century saw the crisis and evolution of this model . The rigid institutions began to break down or intermingle. After World War II especially, new “free-floating” forms of control emerged that did not rely on confining people in fixed spaces . Deleuze coined the term “societies of control” to describe the successor to disciplinary societies . In societies of control, control is continuous, pervasive, and fluid – like a computer program that modulates in real time, rather than a factory clock that rigidly resets each day. He wrote that “enclosures are molds, distinct castings, but controls are a modulation, a self-deforming cast that continuously changes” . The language of control is no longer the clock or the assembly line, but codes and passwords , where individuals are “dividuals” – data points in a network – rather than unified “individuals” molded by one institution at a time . In practical terms, we’ve shifted from a world where every situation had its own closed system (you clock in at the factory, you leave and go home, etc.) to a world where one is never truly outside the reach of control . Corporate and digital networks have replaced the factory as the dominant site of power: the corporation is “a spirit, a gas,” diffused everywhere, and the salary system becomes a continuous tracking of performance rather than a fixed wage . Education too becomes lifelong (continuous training replaces finishing school then being done) . This is evident today: workers are always on email, expected to respond even off-hours (work seeps into home), while personal life is constantly commodified and fed back into one’s professional and consumer profile (your LinkedIn learning courses, your social media presence affecting job prospects, etc.). There is no clear boundary where institutional control stops; control is now woven into the fabric of daily life through information systems . The symbolic architecture of industrial control was epitomized by the factory and the prison – heavy, material, and visible structures. The symbolic architecture of informational control might be better captured by the matrix or the network – light, invisible, dynamic structures. The Matrix (as popularized by9293 9394 9596 97 96 98 99100 101 102 103 12 the film) is an apt metaphor: a simulated reality controlled by code, entrapping human minds without their awareness. While our world is not literally a computer simulation, the metaphor speaks to how power now often operates by shaping our perception of reality (through media, data filtering, etc.) rather than physically confining our bodies. Another powerful symbol is the database or cloud – in a sense, the cloud servers of big tech companies form the “central tower” of a new panopticon, but unlike Bentham’s brick-and-mortar tower , we cannot see it, and it sees through algorithms not human eyes. It is everywhere and nowhere. Moreover , we ourselves carry the apparatus of our surveillance and distraction in our pockets – the smartphone is a personal panopticon (feeding data) and personal soma dispenser (feeding entertainment) in one. In the industrial age, one could at least imagine smashing the machines or breaking out of the prison. In the informational age, how does one smash an algorithm or escape a data network that one depends on for participation in society? The architecture is diffused within us . Philosopher Shoshana Zuboff uses the term “Big Other” to denote the sprawling automated surveillance that now substitutes for “Big Brother” – a decentralized power that works through constant tracking and algorithmic nudges rather than central edicts . People often willingly submit to Big Other (by clicking “I Agree” to data collection, by quantifying their every step or heartbeat via wearables, etc.), which is a hallmark of control society vs disciplinary society – coercion is replaced by coaxing. Another shift in symbolic structure is from the Pyramid to the Rhizome (to use Deleuzian imagery). Old power was hierarchical, top-down (a pyramid of command). New power , though ultimately serving elite interests, often presents itself as distributed, participatory, networked (like a rhizome plant with no clear trunk). Social media networks, for instance, give the appearance of a flat space where everyone can speak, but in truth they channel and amplify certain currents (often aligning with power or profit), functioning like algorithmic “invisible hands” guiding the crowd. The architecture is one of subtle guidance systems rather than open commands. Instead of a boss ordering you, you have a platform nudging you. Instead of a censor erasing a pamphlet, you have an algorithm down-ranking a post. It’s steering rather than shoving , to borrow the terminology of libertarian paternalism. This makes the control architecture less immediately confrontational but perhaps more insidious , because it’s easy to deny it exists (“no one is forcing you – you chose to do x, didn’t you?”). We see this also in policing and social control: rather than mass arrests or martial law (blunt instruments), we have “predictive policing” focusing on micro-areas and specific individuals, data-driven “risk assessments” deciding who gets parole or who doesn’t get a loan, etc. Control fragments and individualizes – a process Deleuze called “dividualization” , where each person’s profile is separately modulated . There’s no need for one-size-fits-all rules when you can have precision targeting : each person can be subtly handled according to their data-defined tendencies (one might never know that one’s feed is slightly sanitized of radical content because the algorithm decided showing it might radicalize you, whereas someone else’s feed is flooded with it to keep them engaged). The mythic shift can be summarized as: from Prometheus chained (physical punishment for stealing fire) to Narcissus chained (psychological capture by one’s reflection). It’s not that overt punishment is gone – it’s that compliance is achieved earlier in the chain of causality , at the level of shaping motivations and perceptions, so that physical force rarely needs to be used. The power structure has learned to delegate the work of suppression to our own psychology .43 99100 13 We can also juxtapose Orwell’s and Huxley’s favored elements : Orwell’s world was metaphorically one of iron and fire – heavy industry, boot camps, torture chambers (the hard elements of earth and fire used against bodies). Huxley’s was one of air and water – airborne soothing chemicals, a society flowing in a state of liquidity and pleasure (everyone “drifting” along happily). Our world’s control architecture uses fire in the sense of high technology and possibly force when needed, but prefers air and water – invisibly spreading its influence (airwaves, wireless signals, cloud computing) and absorbing resistance by dilution (flooding discourse with irrelevant or polarizing content so no coherent opposition forms – drowning truth in information as Huxley warned ). In concrete terms, consider the workplace example: Factory-era control was the foreman with a stopwatch enforcing productivity (Charlie Chaplin’s Modern Times image). Today’s workplace control might be your computer tracking keystrokes, your standing desk reminding you to take a posture break (benevolent surveillance), your Slack status revealing how responsive you are. It’s less personally confrontational but arguably just as total – one is entwined in a web of monitoring that one might even appreciate for “helping” to be more productive or healthy, fulfilling Skinner’s prophecy of people guided by systems that claim to optimize their well-being. Meanwhile, mass behavior is guided by memetics and virality – intangible yet powerful forces that can emerge spontaneously but also be manipulated. The architecture of social media means ideas spread in network cascades rather than through central broadcast alone. This is why controlling the network conditions (the algorithms, the moderation policies) has become the new focus of power struggles, analogous to seizing the radio stations in a 20th century coup. Instead of shutting down a newspaper , a modern government might flood the network with disinformation or amplify certain tropes via bot armies – an attempt to control the narrative from within the network . The industrial symbolic architecture valued order, discipline, and hierarchy , often represented by straight lines, right angles, and rigid schedules. The informational symbolic architecture values flow, flexibility, and responsiveness – represented by feedback loops, data streams, and algorithms that update continuously. Power is exercised by shaping the flows rather than erecting the walls. Yet, one must note: the old architectures do not disappear entirely – they often reinforce the new. For example, the traditional state’s legal and coercive apparatus (courts, police) still exist, but they increasingly lean on or are guided by informational systems (e.g., police use big data to decide whom to monitor; courts struggle with how to handle evidence from digital surveillance). Similarly, corporations still have CEOs and org charts, but day-to-day employee management is heavily mediated by software tools and analytics. In summary, the architecture of control has transformed from the tangible and overt to the intangible and covert . The blueprint of power is no longer carved in stone or cast in iron; it is written in code and propagated through networks. Our challenge is that human instincts to recognize oppression (which might react to a fence or a billy club) are less attuned to lines of code and interface dark patterns . We may not even perceive the architecture that confines our cognition, because it looks like everyday use of the apps and services we enjoy. The chains are made of data – visible only through effort and often presented as jewelry rather than shackles. With this theoretical and narrative foundation laid, we can now turn to the practical question: How does (or should) our legal system respond to this new paradigm of cognitive capture and algorithmic governance? Our final section will delve into U.S. constitutional law and related legal developments to104 14 examine whether our doctrines of liberty, privacy, and rights – forged in the era of industrial government – can adapt to the age of AI and surveillance, and what reforms or interpretations might be emerging to protect cognitive freedom and democratic values in this brave new world. Legal Implications in the Age of AI and Surveillance: A Constitutional Analysis The rise of AI-driven surveillance and cognitive capture poses profound challenges to legal frameworks that were developed for an earlier era. In the United States, the Constitution’s guarantees – of privacy, expression, due process, and equality – are being tested by technologies and practices that the Founders could not have imagined. This section provides an exhaustive legal analysis, grounded in U.S. constitutional law, of how core doctrines are evolving (or need to evolve) in response to digital surveillance and algorithmic governance. It will also consider emerging legislation and comparative perspectives from other jurisdictions, such as the EU, to highlight how law can address the tensions between technological power and individual rights. Privacy and Surveillance: The Fourth Amendment’s Evolution The Fourth Amendment safeguards “[t]he right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures,” generally requiring a warrant for government searches. Traditionally, this was about physical intrusions – rummaging through desks, wiretapping phone lines, etc. Over the 20th century, Fourth Amendment doctrine expanded to cover intangible privacy via the “reasonable expectation of privacy” test from Katz v. United States (1967), which held that wiretapping a phone booth was a search . Katz declared that the Fourth Amendment “protects people, not places,” shifting the focus to whether an individual expected privacy that society is prepared to recognize as reasonable. However , the digital age has strained Katz’s framework. Modern surveillance often involves collection of data exhaust (location pings, metadata, cloud-stored files) by third parties (telecoms, internet providers, tech companies). Under the old third-party doctrine (from cases like Smith v. Maryland (1979)), information voluntarily given to a third party carried no expectation of privacy – e.g., dialed telephone numbers or bank records were not protected, since you “shared” them with the phone company or bank. This doctrine, if applied strictly today, would mean that vast amounts of sensitive digital data – emails, search queries, GPS logs – get zero Fourth Amendment protection because they pass through third-party servers. Recognizing the stakes, the Supreme Court began updating Fourth Amendment law to the digital reality with two landmark cases in the 2010s: Riley v. California (2014): Held that police generally must obtain a warrant before searching a cell phone seized during an arrest . Chief Justice Roberts noted that cell phones “hold for many Americans the privacies of life” – their immense storage of personal data (photos, messages, location history) makes them qualitatively different from physical items like a cigarette pack or wallet. Thus, the traditional search-incident-to-arrest exception (which lets officers search items on an arrestee for weapons or evidence) does not extend to cell phones . Riley was a unanimous recognition that digital data demands heightened privacy protection because of its volume and sensitivity. It underscored that pre-digital exceptions cannot be unthinkingly applied to new tech .105 106 • 107 108 108 109 110 15 Carpenter v. United States (2018): A 5-4 decision requiring a warrant for the government to obtain historical cell-site location information (CSLI) from phone companies . Overturning the rigid third-party rule in this context, Chief Justice Roberts (again authoring) wrote that individuals have a reasonable expectation of privacy in their long-term physical movements as captured by CSLI . The Court recognized that a cell phone’s location logs over weeks create a detailed “chronicle of a person’s physical presence” – effectively a comprehensive dossier of one’s life, which differs in kind from the discrete bits of information in older cases . Thus, even though cell location records are held by a third-party (the telecom), accessing a week or more of such data is a search under the Fourth Amendment. Carpenter essentially carved out an exception to the third-party doctrine for certain digital data , acknowledging that the doctrine’s premise (that sharing with a service means waiving privacy) breaks down when that sharing is unavoidable in modern society and the data is deeply revealing . Together , Riley and Carpenter signal the Court’s awareness that technology necessitates stronger privacy protections . They hesitate to extend pre-digital exceptions wholesale to new scenarios . As the EPIC brief summary notes, these cases show the Court is reluctant to simply apply old rules “developed before cell phones and the internet” to the new world . Instead, there is an emerging principle: if a technology creates a searchable mosaic of a person’s life (as continuous GPS tracking or cell data does) or if it holds unprecedented quantities of personal information (as smartphones do), the Fourth Amendment will likely require a warrant . This is sometimes called the “mosaic theory” in lower courts – the idea that collecting many pieces of data over time is a qualitatively different, more intrusive search than each piece individually. Despite these advances, many areas remain fraught. For example, real-time mass surveillance like ubiquitous camera networks and facial recognition hasn't been squarely addressed by SCOTUS. If the government uses AI to scan every face in public spaces against a watchlist, is that a search? In United States v. Jones (2012), five justices in concurrences suggested that long-term GPS tracking violates privacy expectations even if each momentary observation in public is not private – again a mosaic concept. Facial recognition could be analogized to that. But doctrine is nascent. Lower courts have begun wrestling with things like pole cameras (constant video on a suspect’s home) – some have applied Carpenter’s reasoning to say four weeks of such recording is a search requiring a warrant (the continuous nature intrudes on privacy). Another frontier: NSA-style bulk data collection . The revelation that government collected telephony metadata of millions (under a broad PATRIOT Act interpretation) triggered debates on whether such bulk collection violated the Fourth Amendment or was simply a (legal or illegal) statutory issue. Post-Carpenter , one could argue that bulk collection of, say, internet metadata or location data is indeed a search. Carpenter’s footnotes left open national security surveillance might be treated differently, so that’s unresolved. However , reforms like the USA FREEDOM Act (2015) ended bulk collection of phone metadata, moving to a targeted query system. Notably, the Fourth Amendment currently restrains government actors, not private companies . So the troves of personal data collected by Google, Facebook, Amazon etc., while raising deep privacy issues , are largely outside constitutional purview – they’re governed by consumer protection and privacy laws (if any). But those troves become Fourth Amendment-relevant when the government seeks to access them (via subpoena, warrant, or partnership). Carpenter helps in that latter scenario (requiring warrants for certain data), but it doesn't stop companies themselves from collecting the data in the first place. Thus, the legal• 111 112 111 112 111 112 111 112 107 111 106 107 111 112 16 gap: much surveillance is privatized . For instance, if Amazon’s Ring cameras blanket neighborhoods with privately owned eyes, police can often get that footage without a warrant by consent of the owner (or sometimes via terms that allow police to request it). Likewise, data brokers collect and sell location data from apps – and law enforcement has taken to simply buying such data (circumventing Carpenter’s warrant requirement by purchasing from a broker , an issue now being litigated and addressed in some proposed laws). There is a nascent concept of “digital trespass” revived by Justice Gorsuch and others – analogizing certain data access to a property trespass (the original basis for Fourth Amendment before Katz). If one treated personal data as property or effects, government appropriation of it (even via third party) might be seen as a seizure. Gorsuch in Carpenter dissented in result but mused that maybe we should reconceptualize data as an effect protected by the Fourth Amendment . Indeed, some scholars argue for treating personal data as property to trigger Fourth Amendment and other protections (this bleeds into the property rights discussion below). What is clear is that the Fourth Amendment is undergoing a renaissance in adapting to digital surveillance. “Technological advances make constitutional privacy protections more important than ever,” as an EPIC commentary put it . The Supreme Court is slowly crafting a jurisprudence that refuses to let convenience or outdated doctrines eviscerate privacy. There’s recognition that without intervention, “advancing technology” could erode Fourth Amendment rights , so the courts must ensure the Amendment’s protections “do not become obsolete” (language reminiscent of Justice Sotomayor’s concurrence in Jones, or Butler/Brandeis dissents from Olmstead in 1928 presciently arguing for privacy in communications) . In the coming years, courts will likely confront questions like: - Is geofence warrant (demanding from Google the identities of all devices in a location during a time window) a valid warrant or an unconstitutional general search? - Do people have a reasonable expectation of privacy against AI surveillance in public (like persistent tracking via drones or analytics)? If Carpenter logic is extended, prolonged surveillance even in public may need warrants. - How to treat smart home data (like voice assistant recordings, IoT device logs)? Some cases (e.g., involving Amazon Echo recordings in murder investigations) have tested this – usually requiring a warrant and companies often demand it. The Fourth Amendment will continue to be a critical battleground for maintaining a sphere of privacy and personhood in the face of ubiquitous surveillance tech. It requires courts to be technologically informed and willing to, at times, break from old analogies. As seen, the trend is towards requiring more judicial oversight (warrants) for access to intimate digital data , which is a heartening development for civil liberties. Freedom of Thought and Expression: The First Amendment – Platform Power and Cognitive Liberty The First Amendment declares that “Congress shall make no law…abridging the freedom of speech, or of the press,” which protects not only the right to speak but also, implicitly, the right to receive information and the freedom of thought and belief. In the context of cognitive capture, two distinct First Amendment issues emerge: (1) the power of private platforms to curate (or manipulate) speech and information, and the extent to which that power is or is not constrained by the First Amendment; (2) the notion of cognitive liberty113 114 107 115 116 111 112 17 – whether individuals have (or should have) a constitutional or legal right to mental self-determination free from undue manipulation or intrusion, akin to a right to freedom of thought. Platform Curation and the First Amendment: In the U.S., social media and internet platforms are mostly private companies, not government actors. Therefore, their decisions to moderate content (remove posts, ban users, algorithmically downrank or uprank content) are not bound by the First Amendment in the sense of protecting users’ speech from company interference. In fact, in recent litigation, the opposite argument was made: that the platforms themselves have a First Amendment right to perform editorial curation of user content. This came to a head with laws in Florida and Texas that attempted to prohibit large social media platforms from “censoring” (removing or moderating) users based on viewpoint. Platforms (through trade groups like NetChoice) challenged these laws as violating the platforms’ own First Amendment rights. The Supreme Court in NetChoice LLC v. Moody (the Florida case) and the corresponding Texas case was expected to address this, and as of July 2024, it indeed did in NetChoice (Moody) . As summarized by the lower court and recent commentary, the prevailing view is that social media platforms are analogous to publishers or editors for First Amendment purposes , especially regarding how they curate feeds and content . In a major decision (Moody), the Court effectively held (with a majority opinion by Justice Kagan) that online platforms have a First Amendment right to select, edit, and arrange content and that government cannot compel them to host or promote speech they do not wish to . The Court applied precedents like Miami Herald v. Tornillo (1974) – which protected a newspaper’s right not to carry a political candidate’s reply – to the digital context, concluding that the editorial judgment exercised by platforms (including via algorithms) is protected speech . The Court explicitly noted this protection extends to “how” platforms display content, including algorithmic personalization . In other words, the use of algorithms to curate a personalized news feed is an exercise of editorial discretion protected by the First Amendment . Government interference in that – whether forcing platforms to carry content (as the laws tried to prevent de-platforming) or hypothetically forcing them to remove certain content – is subject to strict scrutiny and likely unconstitutional . This means that, currently, platforms have broad First Amendment immunity from laws regulating their content moderation . Users, conversely, do not have First Amendment rights against platforms (only against government). Attempts by users to claim platforms are “state actors” have failed – e.g., Prager University v. Google (9th Cir . 2020) held YouTube is not a state actor simply because it’s open to public use; Manhattan Community Access Corp. v. Halleck (2019) similarly held a private public-access TV operator not a state actor for 1A purposes . So, a user can’t sue a platform for banning them on free speech grounds. However , the power of platforms over public discourse has raised concerns and alternative approaches. Some scholars and lower courts had toyed with the idea that certain dominant platforms could be seen as “common carriers” or have obligations akin to the public forum doctrine, but the Supreme Court’s stance in Moody strongly rejects that equivalence . The Court distinguished platforms from, say, phone companies (which are content-neutral conduits); instead, platforms are seen as speakers because they make choices about content dissemination . The implication is that First Amendment law currently shields the cognitive capture apparatus when it is privately run. The curated feeds that might bias information diets, the algorithmic amplification of extreme content for engagement – these are private editorial decisions protected from government regulation. On one hand, this prevents heavy-handed government control of online speech (a legitimate concern given the First117 118 117 118 117 118 7576 7576 117 118 119 120 121 122 121 122 18 Amendment’s core anti-censorship purpose). On the other hand, it means our democracy must rely on extra- legal solutions (market pressure, ethical commitments, user empowerment) to address platform-driven distortions of public discourse , since direct regulation (like forcing transparency in algorithms or requiring neutrality) could be struck down as infringing the platforms’ First Amendment rights . There is nuance: not everything a platform does is clearly expressive. Some have argued that certain algorithmic processes might be more like conduct than speech. For example, Justice Thomas mused in a statement that perhaps hosting is more like a common carrier function in some cases. But the majority in Moody doesn’t seem to allow much room there . Interestingly, some justices in early hints (during emergency stay considerations) noted we need to carefully assess how First Amendment applies to algorithms – whether these automated recommendations are “speech” or something else . The final Moody decision indicates they view it as speech if it involves choices about content presentation . Cognitive Liberty and Freedom of Thought: Beyond the platform moderation issue, there is a deeper question of whether the Constitution (particularly the First Amendment) protects the right to freedom of mind – sometimes termed cognitive liberty . This is not a traditional phrase in constitutional law, but it’s emerging in scholarship and future-oriented legal thought. The First Amendment implicitly covers some aspects: for example, it has long protected the right not to speak (compelled speech doctrine) and the right to receive information . The Supreme Court said in Stanley v. Georgia (1969) that the government cannot prohibit a person from possessing obscene material in the privacy of home, intoning that “the Constitution protects the right to receive information and ideas, regardless of their social worth... and to be generally free from government intrusions into one's privacy and control of one's own thoughts.” This hints at a freedom of thought. Justice Jackson in West Virginia v. Barnette (1943) delivered a famous line: “If there is any fixed star in our constitutional constellation, it is that no official... can prescribe what shall be orthodox in politics, nationalism, religion, or other matters of opinion.” . This is a robust defense of intellectual freedom – government can’t force you to say or believe something. However , cognitive capture by private or novel means (like algorithmic manipulation or neurotechnology) is not directly addressed by existing law. Scholars like Nita Farahany have argued that “cognitive liberty” should be recognized as a fundamental right – encompassing both the right to use technologies to expand one’s mind and the right to be protected from unwanted intrusions or disclosures of one’s thoughts . The scenario of brain-computer interfaces or neuro-monitoring at workplaces, for instance, is becoming real. If an employer sought neural data from workers (say, attention metrics from a headset), would the employees have a right to refuse on cognitive liberty grounds? Some states or countries might legislate that (Chile in 2021 moved towards a neuro-rights law). In the U.S., one might ground it in the First Amendment’s freedom of thought or the right to privacy (a penumbral right from various amendments). A recent piece in the First Amendment Law Review by Emma Schambach explicitly calls for using the First Amendment to protect cognitive liberty in face of neuro-surveillance . It suggests the First Amendment, with its historical protection of freedom of thought and belief, can serve as a “most potent shield” against government use of invasive neurotech . The argument is that if the government tried to directly monitor or alter your thoughts, that should be seen as at least as serious as compelled speech or censorship – a violation of the “inner sanctum of the human mind.” Indeed, the piece cites how recent case law like 303 Creative (2023) and others protect individuals’ rights to have their internal beliefs and mental decisions reflected (or not reflected) in their expression . If scanning brainwaves could produce “speech” or reveal thoughts, doing so involuntarily could be cast as compelled speech or a violation of the right not to speak one’s mind.123 124 125 126 125 126 7576 127 128 129 130 128 127 128 127 127 131 132 133 19 American jurisprudence has not yet had a case squarely on “freedom of thought” as such, but historically the Court has said the First Amendment “necessarily protects the right to receive information and ideas” (see Kleindienst v. Mandel (1972)) and arguably a right to mental privacy can be extrapolated. Some have analogized drug laws or certain restrictions as impinging cognitive liberty (e.g., arguments that the ban on psychedelics impedes a right to cognitive exploration; courts haven’t recognized that). But with technology like brain scans, it’s more concrete: a state could try to use AI to analyze your facial expressions or neural patterns to see if you’re lying – that might implicate self-incrimination (Fifth Amendment) as well as mental privacy. In the realm of platforms controlling information flows , cognitive liberty might manifest as concern that if private or government actors manipulate feeds to shape beliefs covertly, is there a violation? If the government coerces platforms to do so, it could be a First Amendment issue (state action). Recent controversies like the government flagging misinformation to social platforms raised First Amendment questions: courts have differed on when suggestion becomes coercion (see the ongoing Missouri v. Biden case, where plaintiffs allege gov. officials pressured social media to remove content, raising state action issues). At present, American law does not explicitly recognize cognitive liberty as a standalone right , but the concept is emerging at the intersection of First Amendment (free thought), Fourth Amendment (privacy of the mind), and due process (mental autonomy) . The UNC law review blog suggests using the First Amendment as a weapon against neuro-surveillance, highlighting that “freedom of thought and expression” are core and could be extended to protect the mind from invasive tech . If, say, police used a brain scan to detect memories without consent, one could argue the First Amendment’s spirit (and the Fifth’s protection from self-incrimination) should prohibit that. The international perspective is noteworthy: The International Covenant on Civil and Political Rights (ICCPR) Article 18 protects “freedom of thought, conscience, and religion” and says freedom of thought is absolute (no derogation). The EU Charter also implicitly covers freedom of thought (Art 10). These may influence how U.S. scholars push this concept. In summary, under First Amendment doctrine: - Private platforms largely have First Amendment protection for their content curation decisions , meaning the government’s ability to regulate cognitive capture by platforms is limited by the platforms’ own speech rights. - There is a growing scholarly push to articulate a right to cognitive liberty – using First Amendment (and perhaps privacy law) to ensure individuals have autonomy over their own minds and are protected from invasive surveillance or manipulation, especially by the government. This is not yet a developed doctrine, but the seeds are in existing case law that recognizes freedom of mind (Barnette’s “no orthodoxy” principle, cases rejecting compelled association or belief, etc.) . Algorithmic Governance: Due Process and Equal Protection As AI systems increasingly make or inform decisions that affect individual rights and opportunities – from criminal sentencing and policing to credit, employment, and welfare determinations – they implicate constitutional guarantees of procedural fairness (Due Process) and nondiscrimination (Equal Protection) . The legal system is grappling with how to ensure that automated or algorithmically-driven decisions can be contested, explained, and corrected when necessary, and how to fit concepts of bias and intent into frameworks largely designed for human decision-makers.128 127 117 75 128 127 20 Procedural Due Process: The Fifth and Fourteenth Amendments prohibit government from depriving any person of life, liberty, or property without due process of law. When algorithms are used by state actors to make decisions – say, a risk assessment tool in sentencing, or an automated system determining eligibility for public benefits – individuals have due process interests in how those decisions are made. A key issue is transparency and the opportunity to be heard when an algorithm affects you. A seminal case in this area is State v. Loomis (Wisconsin Supreme Court 2016) . Loomis was sentenced in part based on a COMPAS risk score (a proprietary algorithm predicting recidivism). He argued this violated his due process rights because he couldn’t scrutinize how COMPAS worked (trade secret), nor challenge whether it was accurate or biased in his case . The Wisconsin court allowed COMPAS’s use but laid out warnings and limitations : judges must not treat the score as deterministic, must consider it alongside other factors, and must acknowledge its secret factors and potential biases . The court stressed COMPAS wasn’t intended for sentencing severity (only post-sentencing management) , and that gender is used in it, which raised concerns. Loomis highlights the due process problem of algorithmic opacity : if a person cannot know why or how an adverse decision was reached, how can they meaningfully contest it? Procedural due process typically requires notice of the reasons for a government decision and some chance to rebut. With AI “black boxes,” reasons may be inscrutable. Scholars like Danielle Citron and Frank Pasquale have advocated “ algorithmic due process ,” arguing for the right to some form of notice/explanation and opportunity to contest algorithmic decisions . This doesn’t necessarily mean revealing source code, but perhaps governments using algorithms must at least explain the factors considered and allow challenges to the data or assumptions. For example, if an automated system denies you unemployment benefits, due process might entitle you to an explanation of the criteria that led to denial and a hearing to correct errors. There have been real fiascos: e.g., Michigan’s “MiDAS” unemployment fraud detection algorithm falsely accused tens of thousands of fraud with no human review, leading to devastation. Courts or settlements later stepped in. Similarly, a judge in Arkansas in 2020 ruled that an algorithm used to allocate Medicaid home care hours violated due process because recipients couldn’t understand or challenge the scoring (the case Harris v. DHS , I believe). The Supreme Court hasn’t directly addressed algorithmic due process, but its mathews v. Eldridge test (1976) for due process – balancing private interest, risk of erroneous deprivation, likely value of additional safeguards, and government interest – could apply. Algorithms might reduce some biases but could introduce errors. If an algorithm is flawed or uses bad data, due process would favor some mechanism to catch errors (maybe human review or an appeal process). Another aspect is notice : if governments use predictive tools (like predicting who to investigate or where to police), individuals may not get notice until after an adverse action (arrest, enhanced sentencing). The concept of “AI bias audits” or external validation is being floated as a way to satisfy due process and administrative law norms ex ante. Equal Protection: The Fourteenth Amendment’s Equal Protection Clause (and Fifth’s implicit equal protection component) prohibits the government from denying people equal protection of the laws. This has been the foundation of anti-discrimination law. But it traditionally requires intentional discrimination by the state or , in limited cases, laws/practices with unjustified disparate impact under certain statutes (not constitutional but statutory, e.g., Title VII in employment or the Fair Housing Act).134 135 136 137 134 135 138 139 140 21 Algorithms present a challenge: they can produce disparate impacts along lines of race, gender , etc., even without intent – often reflecting historical biases in data. For example, a predictive policing algorithm might send police disproportionately to minority neighborhoods because historical crime data (and enforcement patterns) are biased . Or a hiring algorithm might filter out resumes in ways that disadvantage women (as Amazon found with a resume screener that penalized women). Constitutionally, a plaintiff would have to show intentional discrimination to trigger strict scrutiny (Washington v. Davis (1976) principle). Most algorithms aren’t explicitly designed to discriminate (and may not even use protected categories), so intent is hard to prove. Yet the effect can be as damaging as overt bias. There’s burgeoning discussion on whether the law should adapt – perhaps treating some algorithmic biases as de facto intentional once known (i.e., if an agency continues using a tool knowing it’s racially biased, is that tantamount to intentional discrimination?). That’s speculative; courts so far haven’t stretched equal protection that way. However , in statutory realms, there’s movement: e.g., the Equal Credit Opportunity Act and Fair Credit Reporting Act are being interpreted to cover AI decisions. The CFPB has said creditors using black-box algorithms still must provide adverse action notices explaining key reasons for denial (implying even AI must yield to an explanation requirement under ECOA). HUD under the Fair Housing Act experimented with disparate impact claims for algorithmic tenant screening or loan underwriting. One concrete example: New York City passed a law requiring bias audits for automated hiring tools, and Illinois has a law about AI in video interviews requiring notice and consent. While these are not constitutional rules, they reflect legislative attempts to ensure fairness. The EU notably in its GDPR gave a right to not be subject to solely automated decisions with significant effects, and a right to explanation (at least to obtain “meaningful information about the logic”). The proposed EU AI Act will likely forbid discriminatory AI outcomes in high-risk areas or at least require rigorous bias mitigation. From a comparative perspective, European law more readily addresses disparate impact without requiring intent, since EU non-discrimination law (and ECHR jurisprudence) accepts indirect discrimination claims based purely on effect if not justified. The US Equal Protection doctrine remains narrower , but Congress or states could fill gaps with statutes imposing algorithmic accountability and anti-bias requirements. One interesting equal protection angle: If a government algorithm explicitly used sensitive attributes (say a sentencing algorithm that gives higher risk to men vs women – which COMPAS did use gender as a factor), that classification would trigger at least intermediate scrutiny (for gender). In Loomis, one issue was that COMPAS factored gender; the court said it’s okay as long as caution is used . But imagine an algorithm that gave different risk scores by race – that would be blatantly unconstitutional. No one would do that overtly, but proxies could slip in. If challengers could show an algorithm is basically using race by proxy (zip code, social ties) intentionally to achieve a bias, that could be an equal protection violation. Proving that intent is tough unless whistleblowers or clear evidence emerge. In sum , with algorithmic governance, due process demands transparency and contestability , and equal protection demands fairness and lack of invidious bias . The legal system is in early stages of grappling with this: - Due process arguments have found some success (courts ordering disclosure or saying lack of explanation violates rights in some admin law contexts). - Equal protection is mostly hypothetical so far; plaintiffs might avoid the high bar of constitutional claims and use statutory routes (like Title VI for programs receiving federal funds, which allows disparate impact regs, or state anti-discrimination laws). -141 142 143 144 22 There is increasing pressure for algorithmic accountability legislation : at the federal level, bills like the Algorithmic Accountability Act (proposed in 2019, 2022) would require assessments of impacts for certain AI systems. President Biden’s “AI Bill of Rights” blueprint (2022) explicitly lists Algorithmic Discrimination Protections as a principle, calling on agencies and companies to guard against biased outcomes . Legal doctrines like administrative law also become relevant – if agencies deploy algorithms, are they following the Administrative Procedure Act? Some argue an unexplained algorithmic decision is “arbitrary and capricious.” There’s also the question of “property interest” in public benefits: if an algorithm erroneously cuts someone’s welfare, that implicates due process because benefits are a property interest – requiring notice and hearing. A case in Arkansas (2017, Barry v. HHS ) did find due process violation when an algorithm reduced Medicaid hours without proper notice of how. The judiciary is gradually acknowledging these issues: - A US appeals court in Chicago (2019) held that a group of taxi drivers plausibly stated an equal protection claim that the city’s rules (giving Uber a freer hand) harmed them – not algorithm, but shows courts might entertain novel equal protection harm arguments. - A federal judge in Missouri v. Biden (2023) issued a preliminary injunction against Biden administration officials from pressuring social media on content (viewing it as likely coercion violating users’ First Amendment). That touches cognitive liberty indirectly (state-induced private censorship). - If an algorithm denies parole disproportionately to one race, a court might at least demand justification. Bottom line: We are pushing the boundaries of due process and equal protection. To preserve fundamental fairness in automated decision-making: - Due process likely requires at least the right to a basic explanation and human review of consequential automated decisions . Government agencies adopting AI should ensure transparency (even if via summary of algorithm’s logic) and an avenue for appeal. - Equal protection challenges will need creative lawyering or updated standards. In absence of constitutional relief, much is shifting to legislation and policy to prevent algorithmic bias – effectively bringing equal protection values into practice through regulatory means. Property Rights in Data and Digital Assets In an economy where personal data is a key resource, the question arises: do individuals have property rights in their data ? Traditionally, U.S. law has been reluctant to treat personal information as owned by the subject of that information. There’s no general property right to your data under federal law, and courts have often dismissed claims where plaintiffs assert property damage from data breaches, citing that personal data isn’t property in the conventional sense (some exceptions in specific contexts). As one Seventh Circuit decision noted, “federal law recognizes no property right in personal data” . However , momentum is shifting in legal thought. Some scholars and advocates propose a propertization of personal data as a way to give individuals more control or economic stake. The idea would be to allow people to sell or license their data, or to sue for conversion if data is taken without permission. Critics counter that treating data as property could commodify fundamental privacy interests and favor those already powerful (companies could just buy rights cheaply from consumers). Still, there are early glimmers of property concepts: - The Illinois Biometric Information Privacy Act (BIPA) (2008) is often cited as implicitly creating something akin to a property/consent right in biometric data (fingerprints, face scans). It gives individuals a right to sue if their biometric data is taken or used without informed consent. Some courts have even referred to it as protecting a “property or privacy interest” in145 146 139 147 113 114 23 one’s biometric identifiers . Under BIPA, a person’s fingerprint is effectively their own – companies must get permission to collect or store it. Texas and Washington have similar laws, and other states are following. - California’s Consumer Privacy Act (CCPA) (2018) stops short of declaring data property, but it gives rights to access, delete, and prevent sale of personal data. “Sale” is defined broadly, implying consumers have a say in how their data is monetized. California considered but did not include paying consumers for data use. - There have been novel legal arguments: in In re Facebook Internet Tracking (9th Cir . 2020), plaintiffs claimed Facebook’s undisclosed tracking of them was a conversion (a property tort). The court did not squarely rule data is property, but it let some claims like intrusion upon seclusion proceed. Federal courts typically dismiss conversion claims for data, except maybe if data has tangible value and is exclusively taken. - One case, Remijas v. Neiman Marcus Group (7th Cir . 2015), held that victims of a data breach could not claim their stolen data was property because no law recognized it as such . - Legal scholarship notes ancient concepts like Right of Publicity (your likeness, name in commerce) are property-like rights in identity. And trade secrets treat confidential business information as property. Why not personal info? Possibly because with personal data, multiple parties have interests (if data is relational, etc.). Constitutionally , property rights in data could matter in contexts like the Fourth Amendment (if personal data was considered an “effect,” government intrusion into it might be seen as a search/seizure requiring a warrant – Gorsuch’s Carpenter dissent floated this ). Also the Takings Clause could be considered if law compelled disclosure of personal data – though that’s far-fetched currently. Some state constitutions or laws are evolving: - California’s Constitution has a right to privacy, not property, but it’s an approach via rights rather than property per se. - The European approach treats personal data protection as a fundamental right, not property. The GDPR gives individuals strong control rights (access, erase, port) – somewhat like property control but conceptually about dignity and privacy rather than commodification. One emergent notion is data as labor (by thinkers like Jaron Lanier , Glen Weyl) – that users should be paid for the value their data generates. That’s more a policy proposal (e.g., “data dividend”) than legal doctrine, but a couple of states like California have contemplated data dividend funds. There was also a suggestion by then-Professor (now FTC Commissioner) Lina Khan and others to treat big platforms as having “information fiduciaries” responsibilities – not property, but a duty of loyalty with our data. In terms of intellectual property law : U.S. copyright law doesn’t let you copyright raw facts about yourself. So you can’t use IP to claim data ownership unless you do something creative with it. There’s a tricky area with databases – the US has no sui generis database right (Europe does), so scraping data is often legal if no contract or specific law is broken. The hiQ v. LinkedIn case (9th Cir . 2022) allowed scraping public LinkedIn profiles, noting you can’t use Computer Fraud and Abuse Act to block that in certain cases. Therefore, if an individual’s data is misused, they currently typically rely on privacy torts, contract, or specific statutes (like BIPA, or video privacy law, etc.), rather than general property law. Emerging proposals/legislation : - The U.S. Congress has debated federal privacy legislation that could clarify individual rights over data (like the defunct proposed American Data Privacy and Protection Act (ADPPA) in 2022). These usually focus on consent and usage restrictions rather than declaring property. - As148 113 149 113 149 24 noted, states are adding sectoral laws (Illinois BIPA for biometrics, statutes for health data, etc.) giving people rights akin to property control in those contexts. One can argue that recognizing a form of property in personal data could strengthen individuals’ position – they could sell or refuse sale, sue for conversion if stolen, and importantly, collect royalties when others profit from their data . However , there are concerns that it would license exploitation (rich companies buying data rights cheaply from the poor , etc.) . Pamela Samuelson wrote a piece “Privacy As Intellectual Property?” cautioning against it . Interestingly, the Supreme Court did indirectly treat data as property in one context: in Carpenter , it said cell location records are protected even though held by a third party (Carpenter was a Fourth Amendment case, not property, but it kind of gave people an ownership-like interest in their cell phone location history) . Also, Justice Barrett in Cedar Point Nursery v. Hassid (2021, a Takings case) wrote separately musing that maybe intangibles could be protected in takings (like intellectual property) . If intangibles can be property for Takings, maybe personal data could too if a regulation appropriated it. For now, property rights in data remain more an aspirational or academic concept than a recognized legal rule in the US . But we see glimpses: - BIPA’s recognition of biometric control . - Some data breach cases now allow “benefit of the bargain” or “loss of value of data” damages theories, implying data has monetary value tied to person (e.g. In re Marriott Int’l Data Breach (D. Md. 2020) allowed plaintiffs to claim value loss for their personal data stolen). - Consumer attitudes: people increasingly feel “my data is mine” , even if law doesn’t fully reflect that yet. One potential constitutional hook: state law property rights define property for constitutional analysis. So if states pass laws saying individuals have a property interest in certain data, then the Fifth Amendment Takings Clause or Fourth Amendment could incorporate that. Or , a state could amend its laws to treat personal info as personal property (some states like California allow initiative constitutional amendments, conceivably “personal data shall be property of the individual”). Comparatively, the EU explicitly rejects property approach – they consider personal data protection a personality right, inalienable in some sense (you can consent to use but not sell your fundamental control entirely). They also have portability rights which mimic a bit the alienability of data (you can move it). Given surveillance capitalism concerns (companies accumulating massive data wealth from individuals), there is an equity argument for data property: could empower a kind of data ownership economy . However , some warn it could commodify privacy and undermine inherent rights. In legal evolution terms, we may see: - Courts gradually acknowledging personal data has value to the person (e.g., approving more damages claims). - More statutes that create quasi-property entitlements (like needing consent to “sell” data, akin to needing permission to use someone’s property). - Possibly new “data trust” models, where data is treated as if held in trust for individuals (less about property, more about fiduciary duty). In conclusion, property rights in data are not fully realized in U.S. law, but the trajectory is toward giving individuals more proprietary-like claims: - Data privacy laws are giving control rights (which are sticks in the property bundle, like the right to exclude by opting out). - Courts are incrementally recognizing that unauthorized data use can cause cognizable harms. - If the law formally enshrines data property, it would150 151 152 153 111 112 148 25 mark a significant shift, affecting everything from Fourth Amendment analysis to how companies structure data transactions. Legislative and Comparative Responses: Toward Algorithmic Accountability Finally, beyond constitutional doctrine, an array of legislative, regulatory, and comparative law developments are attempting to address the challenges of AI and surveillance in ways more agile than courts might. At the U.S. federal level, comprehensive tech governance has lagged, but notable efforts include: - The proposed Algorithmic Accountability Act (introduced 2019, reintroduced 2022) which would require companies to conduct impact assessments for bias, effectiveness, and privacy when using automated decision systems in critical areas (employment, credit, etc.). While not passed, the bill reflects concern that algorithms need external checking – essentially legislative demand for due process and non-discrimination auditing . - The Online Privacy Act or variations of federal privacy laws (none passed yet) would create baseline privacy rights somewhat akin to GDPR, including possibly rights to access/correct data used in algorithms. - The Facial Recognition and Biometric Technology Moratorium Act (proposed in Congress, e.g. by Markey, 2020) sought to ban federal use of facial recognition until a law is in place – highlighting legislative awareness of Orwellian surveillance risks and a pause to evaluate Fourth Amendment and civil rights implications. The Biden Administration’s “Blueprint for an AI Bill of Rights” (2022) , while not binding, lays out principles : 1. Safe and Effective Systems – AI should be tested for safety (implies due process to avoid harm). 2. Algorithmic Discrimination Protections – AI should not discriminate unlawfully . 3. Data Privacy – individuals should have agency over data and be protected (links to property in data concept). 4. Notice and Explanation – people should know when AI is used and understand outcomes (codifying a due process-like requirement of notice/explanation). 5. Human Alternatives, Consideration, Fallback – opt-out options or human review where appropriate (again due process: a human in the loop if needed). While aspirational, agencies might incorporate these ideals: e.g., the CFPB has invoked “unfair” practices authority to warn that black-box credit models still must provide reasons to consumers . On the state level , various laws: - Illinois BIPA (discussed), very impactful in biometric privacy. - Virginia, Colorado, Connecticut, Utah, and California have passed GDPR-like privacy laws with algorithm provisions (e.g., right to opt out of automated profiling in some cases). - New York City’s bias audit law for hiring algorithms (Local Law 144 of 2021) mandates annual audits for bias in race/gender and that summary results be made public, plus notice to job candidates. Comparatively, the European Union is significantly ahead in formalizing these issues: - The General Data Protection Regulation (GDPR) (2018) provides: - Article 22: right not to be subject to a solely automated decision with significant effects, unless based on consent, contract, or authorized by law with safeguards . Even when allowed, data subjects have right to obtain human intervention and contest the decision. This enshrines a kind of due process and dignity right in face of algorithms . - Recital 71 of GDPR says individuals should have the right to an explanation of algorithmic decisions, leading to debate on the scope of that “explanation” requirement. At minimum, GDPR forces companies to divulge the categories of data and logic used in profiling someone upon request. - GDPR also firmly establishes personal data rights154 155 145 146 156 157 156 157 26 (access, correction, deletion, portability), shifting power somewhat toward individuals (this touches property rights concept but frames it as fundamental rights). - The proposed EU AI Act (likely to be enacted around 2024) will regulate AI by risk levels: - It bans certain AI uses outright (e.g., social scoring by governments – clearly a reaction to the Orwellian Chinese model). - It heavily restricts real-time remote biometric identification in public (with narrow exceptions, requiring judicial authorization) – effectively a partial ban on live facial recognition in public by police, addressing Fourth Amendment-type concerns from a human rights perspective. - High-risk AI systems (like in employment, credit, law enforcement, etc.) must meet strict requirements: risk assessments, transparency, quality data to avoid bias, human oversight, etc. This is a regulatory approach embodying due process and anti-discrimination principles. - The AI Act also requires providers to explain and document algorithms for compliance – creating accountability at design stage. - The EU Digital Services Act (DSA) (effective 2023-2024) imposes transparency on very large online platforms for their algorithms, requiring them to allow audits and explain ranking parameters. Also mandates risk assessments for societal impacts (like disinformation, effect on elections) and mitigation – a response to cognitive capture via Huxleyan means (distraction, propaganda). - The EU Digital Markets Act (DMA) indirectly might curb cognitive capture by forcing certain interoperability and data portability. Other regions: - China , conversely, while employing massive Orwellian surveillance, has oddly passed some laws on algorithms: a regulation in 2022 requires recommendation algorithms to uphold socialist values and not endanger security, and to offer users option to disable recommendations – interestingly a nod to giving users a bit of control (though likely not vigorously enforced for individual rights). - Brazil and India are considering data protection bills that have some AI clauses. - Canada ’s proposed Bill C-27 (2022) includes an Artificial Intelligence and Data Act to regulate high-impact AI systems, including fairness and accountability (similar vein to EU). From a legal values perspective : - The EU tends to treat privacy and personal data as fundamental rights, meaning any intrusion must be justified as necessary and proportional (like a constitutional standard). This can produce stricter outcomes (e.g., the Court of Justice of the EU invalidated the EU-US Privacy Shield in 2020 because US surveillance law was too broad, under EU Charter’s privacy and data protection rights). - The US relies on sectoral laws and constitutional doctrines requiring state action for rights enforcement (except anti-discrimination statutes apply to private sector too). International human rights law also increasingly addresses digital rights: - The UN has recognized privacy as a human right that must be protected online as well as offline (see UN Human Rights Committee General Comment No. 16, and resolutions on the right to privacy in the digital age). - The concept of “freedom of thought” in ICCPR Article 18 is absolute, and some human rights scholars argue this should extend to neuro-digital contexts (some mention this in context of cognitive liberty – e.g., the Geneva Declaration on Neurotechnology 2019). - The Council of Europe is preparing a Convention on AI, Human Rights, and Rule of Law . So, in comparative summary: - Europe : proactive, rights-based regulatory framework for AI and data, aiming to prevent both Orwellian and Huxleyan dystopias through law. Strong enforcement (e.g., data protection authorities can fine companies under GDPR, and they have). - US: piecemeal and reactive, with robust free speech protections that sometimes hamper regulation of private power , but also innovation- centric. Some states filling gaps, courts gradually adjusting old doctrines. - Others : China leveraging tech for control yet ironically passing some regulations (likely more to rein in tech companies than state surveillance). - Global : trend to acknowledge digital rights, but uneven. 27 To conclude this sprawling analysis: our current legal system is at a crossroads. The fundamental concepts of the Constitution – liberty, privacy, equality, property – are being reinterpreted under the pressure of AI and surveillance technologies. Ensuring that constitutional values endure in this new era may require: - Bold judicial reasoning to extend doctrines like the Fourth Amendment to cover digital privacy robustly . - Possibly recognizing new rights (freedom of thought/cognitive liberty) or new applications (treating certain data uses as speech issues or property interests) . - Legislative action to fill voids – establishing guardrails for AI (as the EU is doing) and empowering individuals with rights over their data and algorithmic treatments. - International cooperation on standards, as AI and data flow across borders. The dual dystopian specters of Orwell and Huxley remind us that both coercive power and seductive power of technology must be constrained by law. Just as the Constitution in an earlier age was used to check the brute force of government (through warrants, trials, free speech), it now must be wielded (along with new laws) to check the soft tyranny of algorithmic manipulation and the quiet coercion of omniscient surveillance. The goal: a future where technology serves human freedom and dignity, rather than subverts it – where cognitive liberty is preserved as a cornerstone of what it means to be free in the modern world. Sources: Dehouche, Nassim. Post-Labor Economics: A Systematic Review (2025) – discussing automation, abundance, and distribution in a future without traditional work . Saltelli, Andrea, et al. Science, the Endless Frontier of Regulatory Capture (Futures 2022) – describing “cognitive capture” and instrumentarian power of big tech . Postman, Neil. Amusing Ourselves to Death (1985) – foreword contrasting Orwell’s and Huxley’s dystopias . Schneier , Bruce. The Convergence of Dystopias (various essays) – observation that modern society is an amalgam of 1984 and Brave New World . McLuhan, Marshall. Understanding Media (1964) – Narcissus as narcosis: technology as an extension that numbs perception . Skinner , B.F. Walden Two (1948) – utopian community via behavioral engineering; rejection of free will for social good . Deleuze, Gilles. Postscript on Societies of Control (1990) – describing shift from disciplinary enclosures to continuous control via codes and modulation . EPIC (Electronic Privacy Information Center). Background on Fourth Amendment in Digital Age – summarizing Riley and Carpenter and their significance . Supreme Court of the U.S., Carpenter v. United States , 585 U.S. ___ (2018) – requiring warrant for cell phone location data . Supreme Court of the U.S., Packingham v. North Carolina , 582 U.S. ___ (2017) – (not directly cited above, but recognized importance of social media for speech). Schambach, Emma. “Cognitive Liberty and the First Amendment” (First Amend. L. Rev. 2024) – proposing First Amendment framework to protect against neuro-surveillance . NetChoice LLC v. Moody, 34 F.4th 1196 (11th Cir . 2022) and Supreme Court proceedings (2023/2024) – holding Florida social media law likely violated platforms’ First Amendment right to curate content . State v. Loomis, 881 N.W.2d 749 (Wis. 2016) – upholding use of COMPAS risk score with limitations, highlighting due process issues with secret algorithms .107 109 128 127 • 343032 • 4143 • 51 • 59 • 8277 • 9190 • 9896 • 107 111 • 111 112 • • 128 127 • 117 118 75 • 134 137 28 Illinois Biometric Information Privacy Act , 740 ILCS 14 (2008) – first law to give individuals rights over their biometric data (consent, right to sue) . Remijas v. Neiman Marcus Group, LLC, 794 F.3d 688 (7th Cir . 2015) – noting no federal property right in personal data . White House OSTP, Blueprint for an AI Bill of Rights (Oct. 2022) – policy guidance enumerating principles of safe and rights-respecting AI use . European Union, General Data Protection Regulation (2016) – establishes rights regarding automated decisions and personal data . European Commission, Proposal for an AI Act (2021) – comprehensive AI regulation including bans and requirements for high-risk AI (pending adoption in 2024). European Court of Human Rights and CJEU case law (e.g., Big Brother Watch v. UK (2018) ECHR – mass surveillance violates Article 8 of ECHR; Schrems II (2020) CJEU – invalidated EU-US data transfer deal due to surveillance). Comparative references: e.g., Chile’s constitutional amendment efforts on neuro-rights; China’s Personal Information Protection Law (2021) which, despite state surveillance, gives individuals some rights vis-à-vis companies. (PDF) Post-Labor Economics: A Systematic Review https://www.researchgate.net/publication/391536637_Post-Labor_Economics_A_Systematic_Review (PDF) Science, the endless frontier of regulatory capture https://www.researchgate.net/publication/356429861_Science_the_endless_frontier_of_regulatory_capture Attention economy - Wikipedia https://en.wikipedia.org/wiki/Attention_economy Orwell's 1984 and Huxley's Brave New World, Visions http://8ate.blogspot.com/2008/02/orwells-1984-and-huxleys-brave-new.html 1984 vs. Brave New World: Which Dystopian Future Wins Out? – It's A Long Road https://talktomejohnnie.com/1984-vs-brave-new-world-which-dystopian-future-wins-out/ Brave New World Revisited: 2023 - Sud Alogu - Medium https://sudalo.medium.com/brave-new-world-revisited-2023-91ec852f8ce6 Huxley, Aldous - Brave New World - Entertainment for the Masses https://www.grin.com/document/106155?srsltid=AfmBOopQ7AbRi9DA5mUwl-XXVGPfivjIwHoLKY0Dr7FZsOnQdjmRbXrR Michel Foucault on the Digital Panopticon: Artificial Intelligence, Privacy, and Wearable Technology | by Lilac Draccus Media | Medium https://lilacdraccus.medium.com/michel-foucault-on-the-digital-panopticon-artificial-intelligence-privacy-and- wearable-5cad91926ff4 Moody Decision Confirms First Amendment Protects Online Platforms | Davis Wright Tremaine https://www.dwt.com/insights/2024/07/scotus-moody-ruling-a-win-for-online-platforms On the Narcosis of Narcissus ~ The Imaginative Conservative https://theimaginativeconservative.org/2016/09/technological-self-amputation-mcluhan-narcosis-narcissus-morrissey.html• 148 • 113 • 154 155 145 • 128 127 • • • 1 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 53 54 55 56104 52 59 60 61 62 63 64 65 66 57 58 67 68 69 70 71 72 73 74 75 76117 118 121 122 124 77 78 81 82 83 84 85 86 87 88 29 Prometheus - Wikipedia https://en.wikipedia.org/wiki/Prometheus Prometheus and the Quest for Fire https://sarkologistculture.home.blog/2019/09/07/prometheus-and-the-quest-for-fire/ Walden Two - Wikipedia https://en.wikipedia.org/wiki/Walden_Two Postscript on the Societies of Control | The Anarchist Library https://theanarchistlibrary.org/library/gilles-deleuze-postscript-on-the-societies-of-control Deleuze and Data Systems: The Concept of 'Access' in the Control ... https://medium.com/@seow.isa/deleuze-and-data-systems-the-concept-of-access-in-the-control-society-26cb8c1cf75d Fourth Amendment – EPIC – Electronic Privacy Information Center https://epic.org/issues/privacy-laws/fourth-amendment/ Hazel - FINAL https://lawreview.syr .edu/wp-content/uploads/2020/12/1055-1113-Hazel.pdf Administering the Fourth Amendment in the Digital Age https://constitutioncenter .org/news-debate/special-projects/digital-privacy/the-fourth-amendment-in-the-digital-age The Fourth Amendment and Privacy Risks in the Digital Age https://www.culawreview.org/journal/the-fourth-amendment-and-privacy-risks-in-the-digital-age Social Media and the First Amendment - Free Speech Center https://firstamendment.mtsu.edu/article/social-media/ The First Amendment & Platform Regulation - Epic.org https://epic.org/issues/platform-accountability-governance/first-amendment-and-platform-regulation/ Has the US Supreme Court Made It Harder to Regulate Social Media https://www.cigionline.org/articles/has-the-us-supreme-court-made-it-harder-to-regulate-social-media-or-the-opposite/ “Cognitive Liberty” and The First Amendment as a Weapon in the Fight Against Technological Intrusions of the Mind | First Amendment Law Review https://journals.law.unc.edu/firstamendmentlawreview/cognitive-liberty-and-the-first-amendment-as-a-weapon-in-the-fight- against-technological-intrusions-of-the-mind/ The Battle for Your Brain: A Legal Scholar's Argument for Protecting ... https://judicature.duke.edu/articles/the-battle-for-your-brain-a-legal-scholars-argument-for-protecting-brain-data-and-cognitive- liberty/ What the Legal Drama 'For the People' Teaches Us About AI and Legal Ethics - O'Hagan Meyer https://ohaganmeyer .com/2025/02/06/what-the-legal-drama-for-the-people-teaches-us-about-ai-and-legal-ethics/ Algorithmic Due Process: Mistaken Accountability and Attribution in ... https://jolt.law.harvard.edu/digest/algorithmic-due-process-mistaken-accountability-and-attribution-in-state-v-loomis-1 Frontiers | Algorithmic discrimination: examining its types and regulatory measures with emphasis on US legal practices https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1320277/full79 80 89 90 91 92 93 94 95 96 97 98101 102 103 99100 105 106 107 108 109 110 111 112 113 114 149 115 116 119 120 123 125 126 127 128 131 132 133 129 130 134 135 136 137 138 143 144 139 140 147 141 142 30 Blueprint for an AI Bill of Rights | OSTP | The White House https://bidenwhitehouse.archives.gov/ostp/ai-bill-of-rights/ [PDF] Personal Data as Property Under the Fourth Amendment https://scholarship.shu.edu/cgi/viewcontent.cgi?article=1787&context=shlr [PDF] Privacy As Intellectual Property? by Pamela Samuelson https://people.ischool.berkeley.edu/~pam/papers/privasip_draft.pdf145 146 154 155 156 157 148 150 151 152 153 31