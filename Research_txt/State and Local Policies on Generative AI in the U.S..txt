State and Local Policies on Generative AI in the U.S. State governments – and some city and county authorities – have rapidly stepped into the void of AI governance in the absence of a comprehensive federal law . Nearly every state considered AI bills recently, with 48 states introducing AI legislation in 2025 and 26 states enacting over 75 measures . These policies vary widely, creating a patchwork of rules that reflect differing political priorities and local concerns. Below, we survey the landscape of subnational generative AI policy across key themes: partisan approaches, labor and unions, innovation incentives vs. restrictions, environmental and public health impacts, education and public services, and the role of special interest groups. (Federal initiatives are omitted except where they directly shape state/local action.) Partisan Approaches to Generative AI Policy Democratic-led states have generally pursued proactive regulation and transparency around AI, often emphasizing ethics, consumer protection, and worker rights. For example, California and New York – both with Democratic majorities – have advanced sweeping rules for AI accountability. California enacted multiple AI bills in 2024 focusing on transparency and safety: one new law (SB 942) requires any company providing a popular generative AI system (over 1 million monthly users in-state) to offer a free AI-generated content detection tool and visibly label all AI-generated outputs . Another California law (AB 2355) mandates that political ads featuring AI-altered content carry a clear disclosure that the material has been manipulated . Meanwhile, New York passed a first-in-nation AI transparency law for state agencies , compelling each agency to publicly inventory its automated decision tools and prohibiting those tools from undermining any collective bargaining agreements or eliminating any occupied positions . Democratic lawmakers tout such measures as protecting the public from AI harms while ensuring a “human in the loop” for accountability . Republican-led states , in contrast, have tended to frame AI policy around preventing specific harms and preserving innovation freedom . Rather than broad regulation, many GOP-controlled states target malicious or immoral uses of generative AI while guarding against overregulation of business. For instance, Montana’s legislature (Republican supermajority) enacted a “ Right to Compute ” Act, which not only sets risk-management rules for AI in critical infrastructure but pointedly bars the state from restricting anyone’s right to use computing resources for lawful purposes absent a compelling interest . This reflects a libertarian streak aimed at preempting undue government limits on AI development. Arkansas , another GOP-led state, passed a novel law establishing intellectual property clarity for generative AI content : when someone uses a generative AI tool, the person providing the input is deemed the owner of the output (assuming it doesn’t violate existing copyrights) . This law was intended to encourage AI adoption by securing creators’ ownership rights over AI-generated works . Republican legislators have also leaned into criminalizing destructive AI uses like deception and sexual exploitation. For example, Texas – which in 2019 was first to ban deepfake videos in elections – passed a 2023 law making it a crime to produce or share “deep fake” pornography without the depicted person’s consent (a Class A misdemeanor). Similarly, North Dakota outlawed using an AI-powered robot or device to stalk or harass someone, expanding its1 2 3 4 5 6 7 8 9 10 11 1 stalking statutes to cover AI-driven behaviors . These targeted bans on AI-facilitated crimes have gained bipartisan support in many red states, even as broader AI regulations are sometimes resisted. Importantly, some AI policy issues cut across party lines. Election integrity is one area of broad agreement: both Republicans and Democrats are moving to curb AI-generated misinformation in campaigns. For example, in Arkansas , a Republican-sponsored bill to criminalize “deceptive and injurious” deepfakes in electioneering was introduced alongside a Democratic bill creating civil fines for similar misconduct – a rare concerted bipartisan effort in that state. Virginia’s legislature (politically divided) unanimously approved a proposal requiring any AI-generated content in election ads to include a disclaimer stating the media has been artificially generated or altered . And in North Dakota , GOP lawmakers championed a new law mandating “prominent disclaimers” on any political communication created with AI to impersonate a human . These measures, often passing with support from both parties, show a shared recognition that generative AI should not be allowed to undermine democratic processes. By contrast, comprehensive AI governance has revealed some partisan rifts: notably, Republicans in the U.S. House proposed a 2023 moratorium blocking states from enforcing any new AI regulations for 10 years – a move many Democrats (and even some Senate Republicans) opposed as an overreach into states’ rights . Overall, blue states tend to favor broader guardrails on generative AI (especially around bias and labor), while red states focus on punishing egregious abuses and ensuring government doesn’t “stifle” AI innovation. But on certain threats – election deepfakes, child exploitation, and harassment – there is strong cross-partisan consensus to act against AI-driven harms. Labor Protections and Union Positions The rapid rise of generative AI has fueled concerns about job displacement and workers’ rights, and labor unions have become key players in shaping state policies. A prime example is New York’s recently enacted “Legislative Oversight of Automated Decision-Making in Government” Act (the LOADinG Act) , which was aggressively championed by public-sector unions . This law is explicitly designed to protect public employees from being replaced or undermined by AI systems . It requires state agencies to be transparent about any AI they use, and, critically, it forbids the use of AI tools in ways that would lead to layoffs, reduced hours, wage cuts, or outsourcing of duties performed by human staff . As one section states, an automated system shall not cause “ discharge, displacement or loss of position… or transfer of existing duties and functions ” from state employees to an AI . New York’s AFL-CIO and public employee unions strongly backed this bill to ensure AI augments rather than replaces workers . Union representatives openly argued that “ we need to protect workers from losing their jobs to AI ,” making it clear their support for AI was conditional on ironclad job security guarantees . The result is a law that effectively gives unions veto power over government AI deployments that might cost jobs – a stance reflecting organized labor’s broader push to slow down automation until worker protections are in place. Unions in the entertainment and media industries have likewise driven state-level action. In California, the 2023 contract strikes by Hollywood writers (WGA) and actors (SAG-AFTRA) spotlighted the threat of generative AI to creative jobs – and California legislators responded with new laws to safeguard human creators’ rights. For instance, California AB 2602 (2024) makes it unenforceable for any contract to compel a person to accept the use of their “digital replica” in place of their actual performance . In other words, if an actor or professional signs a contract, the employer cannot invoke a contract clause to replace that person with an AI-generated voice or likeness for new work after Jan 2025 – any such provision is deemed unconscionable and void . This was a direct reaction to studios contemplating using AI doubles of actors; the law ensures a human cannot be contractually displaced by a generative AI copy . Along1213 14 15 1617 18 19 720 6 6 7 21 22 22 2 similar lines, California AB 1836 prohibits commercial use of a deceased performer’s digital likeness without consent from the performer’s estate . This prevents studios or advertisers from, say, generating a hologram or AI avatar of a dead celebrity for new films or ads without authorization – effectively extending post-mortem publicity rights into the AI realm. These measures, supported by SAG- AFTRA, reflect union demands that humans maintain control over their voices, faces, and creative output in the age of AI. Beyond California, other states are listening to labor groups on AI issues. Illinois was an early mover with its AI Video Interview Act (enacted 2019, amended 2021), requiring employers to notify job candidates and obtain consent if AI will be used to analyze video interviews. The law also mandates annual reporting on the demographics of those hired versus rejected by such AI, to detect bias . This kind of legislation – aimed at preventing AI-driven employment discrimination – has since spread. New Jersey in 2025 saw bills to regulate “automated employment decision tools” in hiring and mandate independent bias audits of those algorithms . While still pending, these NJ proposals align with what worker advocates and civil rights groups (like the ACLU) have called for: audits and transparency to ensure AI doesn’t automate bias in hiring and firing. Even New York City implemented Local Law 144 of 2021 (took effect in 2023) requiring bias audits for AI hiring tools used by employers in the city – a move widely supported by anti- discrimination advocates and now being emulated elsewhere . State labor departments and unions are also pushing for whistleblower protections related to AI. In New Jersey , lawmakers unanimously adopted a resolution urging generative AI companies to voluntarily provide whistleblower protections for their employees . The resolution encourages AI firms to pledge that workers who raise ethical concerns or disclose AI-related risks internally will not face retaliation. Although non-binding, it signals policymakers’ sympathy to tech employees (like the Google and OpenAI staff who have spoken up about AI safety) and echoes unions’ call for empowering workers to voice AI concerns without fear . All these examples underscore that labor interests are heavily influencing AI policy : public unions want assurances AI won’t cost jobs, and private-sector workers want guardrails against biased or abusive AI in the workplace. Consequently, many states – especially blue states – are writing AI laws with explicit labor protections, often at unions’ behest . Innovation and Business: Incentives vs. Restrictions State governments are walking a fine line between promoting AI innovation as an economic driver and regulating AI businesses to address risks. On one hand, many states are offering incentives, funding, and friendly policies to attract AI companies and investment . On the other hand, some are imposing new obligations on AI developers and users to protect consumers, which industry groups warn could create 50 different standards. Incentives and Pro-Innovation Moves: A number of states see generative AI as an economic opportunity and are crafting policies to lure AI ventures or build local expertise. For example, North Dakota and West Virginia have explored establishing state AI research centers and grant programs. North Dakota lawmakers in 2023 debated creating an “advanced technology grant fund” and an AI research center at a state university (though a major bill on this failed to pass) . West Virginia , similarly, authorized a Task Force on AI and added a mandate that it identify economic opportunities from AI for the state . Texas , notably a tech-friendly state, is considering one of the most comprehensive AI bills in the country – the Texas Responsible AI Governance Act (TRAIGA) – which, besides its regulatory components, explicitly includes measures to boost innovation . The draft TRAIGA bill (circulated by Rep. Giovanni Capriglione in23 24 2526 27 28 729 3031 32 3 late 2024) proposes creating an “AI Regulatory Sandbox” allowing approved companies to pilot AI systems with temporary exemption from certain laws . It also would establish an AI Workforce Training Grant Program and a standing “AI Advisory Council” to guide policy and help industries comply . These provisions aim to make Texas a hub for AI development by balancing oversight with flexibility for businesses. We see similar pro-business positioning in New Jersey , where a pending bill seeks to launch a “Next New Jersey ” initiative to drive AI investments and R&D in the state . And many states continue to offer tax breaks for data centers – which are crucial infrastructure for AI – effectively subsidizing companies that build AI computing facilities locally . (In 2021–22, states from Virginia to Arizona passed data center incentive packages; now some are tailoring these to “AI data centers” specifically, though with mixed legislative success .) Regulatory Requirements on AI Businesses: At the same time, states are increasingly placing compliance burdens on AI providers in the name of consumer protection and transparency. Colorado broke ground with the Colorado AI Act (SB 24-205) , signed in May 2024 as the first broad statewide AI law . It requires developers of “high-risk” AI systems to exercise reasonable care to avoid harming consumers and to prevent algorithmic discrimination , and it grants individuals certain rights (like the ability to opt out of purely AI-made decisions in some contexts) . Under Colorado’s law, companies deploying high- impact AI must perform impact assessments and can be penalized by the state Attorney General for violations . Following a similar track, the proposed Texas TRAIGA bill would ban “unacceptable-risk” AI systems outright , such as AI used for social scoring, manipulative behavioral targeting, or illicit biometric surveillance . It also would force AI developers to keep detailed records of the data used to train generative models (echoing NIST AI risk management guidance) . These record-keeping and risk assessment mandates mark a new level of scrutiny on AI companies at the state level. Some states are targeting specific transparency gaps : Nevada introduced a bold proposal in 2025 to require any company offering AI services in the state to register with the consumer protection office and disclose information about its data storage and usage . The same Nevada bill would force search engine companies to allow users to opt in (rather than be defaulted in) to any AI-generated search results or summaries , ensuring consumers aren’t unknowingly interacting with AI content . It further mandates that AI providers implement frameworks to address “misinformation, fraud, hate speech and bias” in their systems – effectively compelling companies to police their algorithms for harmful outputs. In California , besides the content disclosure law mentioned earlier , lawmakers passed SB 302 (2024) which requires AI chatbot providers to enable easy user opt-out of AI interactions and to verify parental consent if a user is a known minor (note: SB 302 was pending governor’s signature as of late 2024 ). And California’s Privacy Protection Agency has begun drafting rules that would treat certain AI and automated decision-making tools akin to extensions of privacy law – possibly requiring algorithmic impact assessments and cybersecurity audits for businesses deploying AI . Another emerging area is intellectual property and content provenance . We saw Arkansas address IP by assigning output ownership to AI users . Meanwhile, California’s pending AB 1394 (2024) sought to make social media companies label AI-generated images of real people to curb deepfake impersonations – part of a broader trend of “watermarking” requirements. Although not all such bills pass, the trajectory is toward more duties on AI developers to label and trace AI content , so consumers know what they are seeing or hearing. It’s worth noting that industry groups have pushed back on some of these state measures, warning of a compliance nightmare if every state sets different AI rules. The patchwork is already evident: one state3334 35 36 3738 39 40 4033 40 4142 43 44 45 46 47 48 49 9 4 demands AI output disclosure , another says keep training data logs , another might require bias audits – and Congress has even considered preempting state AI laws to avoid this complexity . As of 2025, no federal preemption exists, so AI businesses must navigate each relevant state’s mandates. Some states are consciously coordinating: New Jersey’s clean-energy requirement for AI data centers is triggered only if neighboring states enact similar rules (to level the playing field) . But generally, states are plowing ahead according to local priorities. In short, states are both enticing and constraining AI business : offering funding or tax relief on one hand, and imposing transparency, safety, and anti-bias rules on the other . Companies are being held to new standards of accountability in exchange for the privilege of operating advanced AI within these jurisdictions. Environmental and Public Health Implications Environmental Impact: Energy and Water Use of AI The surge of generative AI has a significant environmental footprint , and state policymakers – especially in states with big data center industries – are responding. Generative AI model training and deployment require massive computing power , translating to high electricity consumption and substantial water usage for cooling server farms . States with clean energy goals are increasingly worried that unchecked AI data center growth could sabotage their climate targets and raise residents’ utility bills. New Jersey and Virginia have emerged as focal points in this debate. In New Jersey, the chair of the Senate Environment Committee sounded an alarm that “ We’re going to have tremendous stress from AI ” on the electric grid, warning of “outrageous [rate] increases” if a wave of new AI data centers comes online drawing power from the existing grid . He authored a bill to require that any new AI-focused data centers procure or generate new clean energy equivalent to their consumption – essentially mandating additional renewable power so that other ratepayers aren’t stuck with higher costs or higher emissions . The bill cleverly triggers this requirement only if other Northeastern states enact similar laws, to avoid driving data center projects to a neighboring state with no such rule . In Virginia, which boasts the world’s largest cluster of data centers (“Data Center Alley”), legislators commissioned a comprehensive study of the industry’s impact . The findings were stark: if AI-driven demand for computing expands as projected, Virginia’s overall electricity usage could grow 183% by 2040 , versus ~15% growth if data center expansion leveled off . This would necessitate building many new power plants and grid infrastructure, costs that all customers would bear . In response, a Virginia bill was introduced to tie the state’s generous data center tax incentives to meeting minimum energy efficiency standards , so that only efficient facilities get the tax breaks . (That proposal did not advance in the 2023–24 session, illustrating the influence of the tech industry lobby in Virginia .) Nonetheless, the conversation has shifted. Even traditionally pro-business states are considering environmental guardrails for AI operations : e.g., Oregon legislators debated ensuring that data centers pay for the grid upgrades they necessitate, rather than passing costs to consumers . And Connecticut saw a 2024 bill (HB 5076) to impose energy and water efficiency requirements on AI data centers and require regular reports to the state’s energy commissioner on their resource usage . Although that bill failed, it was part of a broader trend – the National Caucus of Environmental Legislators noted roughly a dozen bills in 2023– 25 aimed at keeping AI data centers’ resource demands in check . California, ever a bellwether , folded AI into its climate and resource planning via proposed legislation. A California Assembly bill would direct state agencies to develop guidelines for sustainable AI infrastructure , ensuring that the tech sector’s growing water and power needs remain “consistent with3 43 18 50 5152 53 54 50 55 56 57 58 5960 61 62 5 urban water use objectives” and state energy goals . Another California bill, AB 222 , explicitly targets generative AI developers : it would require any company training a large model and operating in California to calculate and report the total energy used in development and what portion came from in-state energy . This kind of transparency mandate – essentially carbon-footprint reporting for AI – is novel. If enacted, it could pressure AI firms to use cleaner energy or locate in states with renewable power to avoid reputational harm. It’s clear that environmental advocates (and some utility regulators) are inserting themselves into the AI policy arena, framing rampant compute power usage as a climate and equity issue. States like New Jersey argue that without intervention, AI data centers could undermine clean energy progress and burden ordinary ratepayers . The Data Center industry , for its part, is pushing back. A trade group representative in one state hearing cautioned that singling out data centers “ risks creating unjustified distinctions ” among power users and that AI facilities shouldn’t be blamed for broader growth in electricity demand . Despite these objections, we can expect more states to propose green AI requirements – from renewable energy quotas and efficiency standards to obligatory disclosures of energy/ water usage – as generative AI adoption swells. Public Health and Safety: AI in Healthcare, Misinformation, and Harmful Content Generative AI is also raising public health and safety concerns , leading states to intervene in how AI is used in sensitive domains like healthcare, as well as to outlaw certain AI-generated content that endangers the public. One major focus has been healthcare and medicine . Policymakers worry that if generative AI is used to communicate medical information or make decisions, it must be done carefully to avoid misdiagnosis, privacy breaches, or erosion of trust in doctors. California’s AB 3030 (2024) squarely addresses this: it regulates the use of genAI in health care provision , requiring that any AI-generated communication about a patient’s health status be disclosed to the patient along with a clear option to follow up with a human professional . In practice, if a hospital uses an AI system to draft, say, a radiology report or a patient instruction, the patient must be notified that AI helped create it, and given a contact for a human clinician . The law carves out reasonable exceptions – if a licensed provider has already reviewed and approved the AI-generated text, no disclosure is needed (the assumption being the provider takes responsibility), and AI that only handles administrative tasks like appointment scheduling is exempt . California’s aim is to ensure transparency and a human safeguard when AI enters the clinical loop, so patients aren’t unknowingly relying on a chatbot’s words about their health. Other states are taking different approaches to protect patients. Arizona lawmakers introduced HB 2175 (2025) to prohibit health insurers and providers from using AI to deny insurance claims or prior authorizations . This was motivated by fears that insurers might deploy black-box algorithms to automatically reject treatments as not “medically necessary.” The bill would force a human review in utilization management decisions, ensuring an algorithm can’t have the final say in denying care . In Arkansas , a pending proposal (H 1816, 2025) sought to ban healthcare providers from using AI in delivering care or generating medical records unless certain criteria are met . While that bill hadn’t passed, it reflected anxieties that unvetted AI might slip into exam rooms or record-keeping, potentially compromising quality or privacy. Additionally, states like Illinois and California are moving to stop AI- driven bias in healthcare: California has a bill barring health insurers from using algorithms that result in discrimination in coverage or pricing (e.g. an AI cannot charge higher premiums or deny coverage in a biased way). All these efforts underscore a principle emerging in state policy: AI can assist in healthcare, but it must not replace human judgment or violate patients’ rights. Transparency, accountability, and the primacy of human clinicians are becoming legal requirements.63 64 6253 65 66 67 68 69 70 71 72 6 Public health is also implicated in the misinformation and psychological harms that generative AI can produce. Several states view widespread deepfakes and AI-generated falsehoods as a threat to societal well- being – sometimes even framing it as a public safety issue. We discussed election deepfake laws earlier; beyond elections, some states are broadening penalties for malicious digital impersonation . For example, New Jersey has a bill to extend the crime of identity theft to include “fraudulent impersonation or false depiction by means of AI or deepfake technology.” This would criminalize scenarios like using an AI voice clone to trick someone into sending money. On the more extreme end, non-consensual sexual imagery generated by AI has been a flashpoint. By 2024, at least four states – including Virginia, Texas, California, and Arkansas – explicitly criminalized the creation or distribution of AI-generated pornography depicting real people without consent , often treating it as a serious misdemeanor or adding it to voyeurism/harassment statutes . In Texas, as noted, the law covers deepfake sexual videos with intent to harm a person’s reputation . Arkansas updated its code in 2023 so that its definition of child pornography now includes AI-generated imagery “indistinguishable” from a real child – closing a potential loophole where offenders could claim “no actual child was harmed” . North Dakota and West Virginia have enacted similar provisions making computer-generated child sexual abuse material fully illegal and punishable as if it were real . These laws were driven by child protection advocates and law enforcement, stressing that AI should not become a tool to circumvent obscenity and exploitation laws. There’s also a mental health aspect: educators and pediatric experts worry about AI’s influence on youth. California’s legislature even considered a resolution endorsing the 23 Asilomar AI Principles – which include safety and ethics guidelines – as state policy . And a California commission was tasked with examining social media and AI impacts on youth mental health , to develop a strategy to mitigate harms like addiction or distorted body image exacerbated by AI-driven content feeds . While not a direct regulation on generative AI, this reflects a growing recognition at the state level that AI’s public health effects (from deepfake bullying to AI-fueled social media) need addressing . In summary, states are beginning to treat some outputs of generative AI as a public hazard – whether it’s false content that could incite panic or undermine democracy, or synthetic pornography that traumatizes victims, or opaque medical AI that could risk patients’ lives. The policy responses range from disclosure requirements and usage restrictions in critical fields (healthcare, finance) to outright criminal bans on the most dangerous AI-generated content . Generative AI’s potential for harm has moved from hypothetical to real for state legislators, and they are rapidly updating laws to keep such harms in check. Education and Public Services Integration State and local governments are also grappling with how to integrate generative AI into schools and public services – balancing innovation with caution. In education, the initial reaction in late 2022/early 2023 to tools like ChatGPT was often alarm: many school districts banned student use of AI to prevent cheating. However , this has evolved toward a more nuanced approach of teach and manage rather than outright prohibit. A notable example is New York City’s public school system , the nation’s largest. NYC initially blocked access to ChatGPT on school devices, but by May 2023 the Department of Education reversed the ban, with the schools chancellor acknowledging a “mindset shift” and a determination to “embrace [AI’s] potential” in the classroom . The district is now developing AI curricula and teacher training, even creating a citywide high school course on AI literacy . This pivot – from ban to integration – is being watched by other large districts. Likewise, some state education boards are formulating guidelines to help teachers use generative AI as a teaching aid (for lesson planning or tutoring) while also educating students on AI ethics and appropriate use. West Virginia lawmakers considered a resolution to study adding a73 1174 11 75 7677 78 79 80 81 7 dedicated AI elective course in high schools , signaling interest in preparing students for an AI-infused world. Although that measure didn’t pass, other states are likely to introduce AI curriculum mandates, much as they did for computer science in the past. In higher education, public universities are also setting policies. Some state university systems (e.g., University of Texas) initially warned against generative AI use for assignments but are now researching best practices for academic integrity in the AI era. We are seeing early moves to update academic honor codes to cover misuse of AI and to invest in tools that can detect AI-generated essays – though these detection tools are imperfect. No state has yet passed a law regulating student use of AI, but many have convened task forces on the “future of education with AI” (for example, California’s state university system hosted symposiums on ChatGPT’s impact on learning, likely informing future policy). City governments are more directly experimenting with generative AI in public service delivery . New York City has emerged as a leader in this space: in October 2023, Mayor Eric Adams released a comprehensive “NYC AI Action Plan” for city government . It lays out a framework for agencies to vet AI tools for risks (bias, privacy) before deployment , to train city employees in AI concepts, and to implement AI solutions that improve services while protecting equity and privacy . As a tangible step, NYC launched a pilot citywide AI chatbot within its “MyCity” online portal . This chatbot is designed to help small business owners navigate city regulations and services by conversing in plain language – essentially providing 24/7 virtual assistance on permitting, licenses, and resources . City officials tout that the AI chatbot can quickly direct entrepreneurs to the information they need across 2,000+ NYC business web pages, lowering barriers for those unfamiliar with government bureaucracy . However , the rollout is cautious: the chatbot is labeled as AI, its answers are sourced from vetted city data, and there’s monitoring in place to correct errors. NYC’s approach exemplifies how a major liberal city is embracing AI to modernize public service (improving access and efficiency), but doing so with a declared commitment to “responsible AI” – meaning no untested algorithms making high-stakes decisions, and lots of human oversight . On the West Coast, San Francisco has likewise leaned into AI for government – appropriate given its position at the tech frontier . In mid-2023, San Francisco became one of the first cities to issue detailed guidelines for city employees on using generative AI tools like ChatGPT . The SF guidelines instruct all city staff and contractors on three key rules: (1) Always fact-check AI outputs, (2) Disclose when content was generated with AI, and (3) Never input sensitive or personal data into a public AI tool . In essence, SF is allowing its workers to leverage AI to draft emails, summarize reports, write code, etc., but reminding them that they are ultimately responsible for the accuracy of the work product . Employees must treat AI as an assistant and double-check its work, and if any AI-generated text or image is used externally, they need to inform the recipient that AI was involved . By prohibiting entry of confidential data into AI, the city is guarding against privacy leaks (since AI services often retain user inputs). San Francisco’s policy aligns with general “responsible use” principles and shows local governments can quickly adapt internal policies to new tech – even absent state or federal mandates. Other public service domains are also testing generative AI. Some city 311 help lines and state agency call centers have piloted AI chatbots to answer common citizen questions. For instance, New Orleans debuted “ChatNOLA,” an AI-driven 311 chatbot that can help residents file service requests or get updates, in partnership with a tech startup . Early reports suggest it eased call volumes and provided multilingual support. At the state level, Pennsylvania ran a pilot using ChatGPT to assist state employees in writing routine documents and answering public inquiries, studying how it might streamline operations (while noting where it made mistakes) . In law enforcement, a few local police departments have even explored82 83 8485 86 87 87 8588 89 90 91 90 92 93 8 AI for investigative support – e.g., using DALL-E or similar to generate composite sketches from witness descriptions – though this remains controversial and not widespread policy. One important area at the intersection of public service and safety is law enforcement’s use of AI . While not generative AI per se, some cities have considered AI analysis of body-cam footage or using robots/ drones with AI. For generative tech, the concerns are more about deepfakes being used against public safety (e.g., fake emergency calls or spoofed police orders). A few states have passed laws to maintain human control in policing: North Dakota’s new law on police robots stipulates no robot can deploy lethal force autonomously (only via remote human control in limited scenarios) . This arose partly from debates in San Francisco, where in late 2022 the city briefly approved police use of robots for lethal force, sparking public outcry and reversal – highlighting the sensitivity of AI/robotics in policing. In sum, integration of generative AI into government and schools is underway but carefully managed . Large cities like NYC and SF are pioneering internal guidelines and pilot programs that could become models for others. Schools are shifting from banning to educating, aiming to produce AI-literate graduates who know how to use these tools ethically. The public sector sees the promise of AI (faster customer service, cost savings, better analytics), yet officials are proceeding with caution, implementing oversight and transparency measures to retain public trust. We can expect more formal state-level guidance on AI use in education and government in the coming years, likely building on the early adopters’ experiences. Influence of Special Interest Groups Finally, it’s important to recognize how special interest groups and stakeholders are driving subnational AI policies from behind the scenes. A diverse array of interests – tech industry lobbyists, civil liberties organizations, trade associations, and grassroots activists – have all played roles in shaping generative AI legislation. Tech Industry and Business Lobbies: Major tech companies and industry coalitions are deeply involved in state AI policymaking, often advocating for light-touch regulation. We saw this in Virginia and other data center-heavy states, where the Data Center Coalition (representing companies like Google, Meta, Microsoft) has lobbied against strict energy rules and successfully watered down or stalled some bills . Similarly, in California, tech firms engaged intensely on AI bills in 2023: Governor Gavin Newsom even cited industry concerns about stifling innovation when he hesitated on signing certain AI transparency bills (though the legislature passed them with broad support) . In Texas, Rep. Capriglione deliberately drafted the TRAIGA bill with input from industry stakeholders over nearly a year . He sought feedback from tech companies, business groups, and others to ensure the proposed rules (like record-keeping and sandbox provisions) would be practical and not drive AI firms out of Texas . This collaborative approach suggests the tech lobby in some conservative states is willing to accept reasonable guardrails – perhaps to preempt more draconian measures – as long as they have a seat at the table. Business groups are also pushing for uniformity : the U.S. Chamber of Commerce and others have quietly supported federal efforts to preempt state AI laws (such as the House Republican moratorium proposal) to avoid compliance nightmares across 50 states . Where preemption is unlikely, they focus on aligning state laws with flexible frameworks like NIST’s AI Risk Management Framework, which many state bills (e.g., Montana’s Right to Compute Act) explicitly reference .9495 96 65 57 4997 98 99 18 100 9 Civil Liberties and Digital Rights Advocates: Groups like the ACLU, EFF (Electronic Frontier Foundation), and EPIC (Electronic Privacy Information Center) have been vocal in state hearings about AI’s risks to privacy, free expression, and fairness. Their influence is evident in provisions such as banning AI “social scoring” (a civil liberties concern drawn from China’s system) in the Texas draft law , or requiring algorithmic fairness audits (a key ask of civil rights advocates) in hiring tool regulations in New Jersey and New York . The inclusion of private rights of action in some state AI bills – allowing individuals to sue for harms – is often pushed by consumer and privacy advocates (though opposed by industry). For instance, an early version of California’s AB 331 (an AI accountability bill) included a robust private lawsuit provision, which was controversial . The final fate of that bill is unclear , but such provisions indicate influence from the plaintiff’s bar and consumer rights groups seeking strong enforcement mechanisms. Labor Unions and Professional Associations: We covered unions’ role in labor protections. It’s worth adding that teacher unions and professional associations (like the American Medical Association, Bar associations, etc.) are also weighing in. The American Federation of Teachers (AFT) , for example, has issued statements on AI in education, cautioning against techno-solutionism and emphasizing the need for teacher training and student data privacy. While not a legislative body, AFT’s stance likely informed some states’ cautious approach to classroom AI use. Medical boards and associations have likewise influenced laws like California’s AB 3030 – doctors’ groups generally supported requiring disclosure of AI involvement to patients, aligning with their ethic of informed consent in care . Issue-Based Advocacy Groups: Specific issues around generative AI have their own champions. Consider the laws about deepfake pornography – these were often propelled by women’s rights and anti-exploitation advocates. After some high-profile incidents of non-consensual deepfake nudes going viral, organizations campaigning against revenge porn and sexual harassment pressed states to update laws. Virginia’s pioneering 2019 ban on deepfake porn was heavily backed by such advocacy, and that template spread to Texas, California, and others . Another example: the Anti-Defamation League (ADL) and other anti-hate groups have been educating lawmakers on how generative AI can produce hate speech or deepfake propaganda, urging inclusion of anti-bias and anti-deepfake clauses in legislation. Nevada’s requirement for AI firms to address hate speech in their systems echoes ADL recommendations for responsible AI practices. Meanwhile, law enforcement agencies and prosecutors’ associations have quietly pushed for statutes to handle AI-driven fraud and false reports (there have been cases of AI-generated swatting calls and voice scams). The result is bills like New Jersey’s AI-augmented identity theft law and various state studies on preventing AI financial fraud . Grassroots and Local Activists: At the municipal level, community activists have shaped AI policy too – often to limit surveillance or police use of AI. For instance, in 2020 the city of Oakland, CA (with strong activist pressure) banned city use of predictive policing algorithms and facial recognition. While not generative AI, this reflects a general skepticism in some communities about government AI. In San Francisco, the brief approval of robot police force mentioned earlier was reversed after public protests . And the Stop LAPD Spying Coalition in Los Angeles has been campaigning against all forms of “data-driven policing,” including any use of AI for suspect imaging or analysis . These local advocacy efforts create a climate in which city officials are very careful about introducing generative AI in public safety contexts. It’s telling that no U.S. city has yet deployed an AI system to generate suspect sketches or do predictive crime mapping without encountering pushback – special interest groups on privacy and racial justice are vigilant. Inter-state Policy Networks: Special interests also coordinate across states. The National Conference of State Legislatures (NCSL) , while nonpartisan, acts as a conduit for model legislation and ideas. Similarly,41 25101 102 103 67 11 46 73104 105 10 the National Association of Attorneys General held sessions on AI, as AGs are eyeing consumer protection angles. And the previously mentioned National Caucus of Environmental Legislators is helping green-minded lawmakers share strategies to deal with AI data centers’ footprint . These networks amplify the influence of interest groups by spreading policies from one state to another (for example, a tech ethics group might draft a model deepfake disclaimer law, which sympathetic lawmakers then introduce in multiple states). In conclusion, virtually every generative AI policy at the state or city level has fingerprints of one special interest or another . Unions fought for and won worker safeguards in New York . Tech companies are shaping sandbox and preemption policies to avoid a regulatory patchwork . Advocacy groups for children’s safety, election integrity, and civil rights have prompted laws on deepfakes and algorithmic fairness. These stakeholders don’t always get everything they want, but they have undeniably steered the conversation. The resulting state laws are thus a reflection not only of elected officials’ priorities but also of which voices – corporate, labor, advocacy, grassroots – have been most effective in making their case in state capitols and city halls. Conclusion From coast to coast, subnational governments in the U.S. are actively shaping the contours of generative AI’s future . In the absence of federal legislation, states and cities have become laboratories for AI governance – implementing rules that range from bold and comprehensive to narrow and precautionary. We see Democratic regions pioneering transparency, risk assessments, and workforce protections , while Republican-led states champion individual liberties, property rights in AI creations, and crackdowns on AI abuse , with considerable overlap on issues like election misinformation and protecting children. Labor unions have secured unprecedented guarantees that AI will not steamroll workers’ livelihoods , even as tech industry players work to keep regulations innovation-friendly . States are also balancing a welcome mat for AI businesses (grants, sandboxes, tax credits) with new obligations (disclosure tools , audits, and usage restrictions) to ensure AI develops responsibly. Crucially, these policies acknowledge that generative AI is not only a business or tech issue, but a matter of public welfare – impacting everything from the environment (power grids straining under AI data centers ) to public health (patients needing to know if AI helped write their diagnosis ) to education (students learning to work alongside AI) and government services (chatbots in city portals ). The involvement of special interests – from unions to civil rights coalitions – has ensured a wide array of perspectives are informing lawmaking, making the emerging regulatory patchwork as diverse as America itself. For practitioners and scholars, this evolving mosaic of state and municipal AI laws offers rich lessons. It shows what concerns resonate most locally (e.g. Hollywood’s influence in CA yielding digital replica laws , or rural states focusing on AI harassment and hunting regulations) and how local values (like a strong union tradition in New York, or libertarian streak in Montana) translate into AI governance. As we move forward, we can expect continued cross-pollination of ideas – and likely increasing pressure for some federal baseline to reconcile the patchwork. Until then, the panoptic view presented here demonstrates that subnational governments are not waiting: they are already in the vanguard, empirically grounding AI policy in real-world concerns and refining their approaches as generative AI’s role in society grows.62 29 106 18 6 33 3 56 67 87 22 11 Sources: National Conference of State Legislatures – Artificial Intelligence 2025 Legislation (state bill summaries) Bryan Cave Leighton Paisner LLP – US state-by-state AI legislation snapshot (analysis of enacted/ proposed state AI laws) White & Case – Tracking the Rise of State AI Laws in 2025 and Campaigns & Elections – States Making Big Moves Toward AI Regulation in 2025 (insights on partisan trends and election- focused laws) Empire Center – “Unions Reprogram NYS To Do Less With More” (commentary on NY’s union-driven AI law) City of New York – NYC AI Action Plan Press Release (2023) ; City of San Francisco – Generative AI Guidelines for City Staff Stateline/Pew Trusts – Lawmakers fear AI data centers will drive up power bills (environmental impacts and state proposals) Broden & Mickelsen LLP – Is Deep Fake Pornography Illegal in Texas? (overview of Texas SB 1361 and SB 751) MultiState Associates – 2024 AI Legislation Tracking (statistics on state AI bills) , and various state legislative records . From California to Kentucky: Tracking the Rise of State AI Laws in 2025 | White & Case LLP https://www.whitecase.com/insight-alert/california-kentucky-tracking-rise-state-ai-laws-2025 Artificial Intelligence 2025 Legislation https://www.ncsl.org/technology-and-communication/artificial-intelligence-2025-legislation US state-by-state AI legislation snapshot | BCLP - Bryan Cave Leighton Paisner https://www.bclplaw.com/en-US/events-insights-news/us-state-by-state-artificial-intelligence-legislation-snapshot.html Steven Otis - Assembly District 91 |Assembly Member Directory | New York State Assembly https://nyassembly.gov/mem/Steven-Otis/story/112599 Unions Reprogram NYS To Do Less With More - Empire Center for Public Policy https://www.empirecenter .org/publications/unions-reprogram-nys-to-do-less-with-more/ AI Deep Fake Pornography in Texas: Is it Illegal? https://www.brodenmickelsen.com/blog/is-deep-fake-pornography-illegal-in-texas/ Here Are the States Making Big Moves Toward AI Regulation in 2025 - Campaigns & Elections https://campaignsandelections.com/industry-news/the-states-making-ai-moves-in-2025/ House Republicans include a 10-year ban on US states regulating ... https://apnews.com/article/ai-regulation-state-moratorium-congress-39d1c8a0758ffe0242283bb82f66d51a A bid to bar states from regulating AI is getting pushback https://www.washingtonpost.com/politics/2025/05/22/state-ai-laws-moratorium-pushback-blackburn/• 2 28 9 75 • 66 3 22 23 • 1 49 14 15 • 20 6 • 83 87 90 • 53 56 • 11 • 107 16 100 124 49 97 2 8 910 12 13 16 25 26 28 30 31 32 36 39 63 64 71 73 74 75 76 77 82 96100 104 3 422 23 66 67 68 69 70102 103 5 7101 620 21 29 11 14 15 17 27 44 45 46 18 19 12 Texas Legislature to Consider Sweeping AI Legislation in 2025 | Inside Global Tech https://www.insideglobaltech.com/2024/11/13/texas-legislature-to-consider-sweeping-ai-legislation-in-2025/ Lawmakers fear AI data centers will drive up residents’ power bills • Stateline https://stateline.org/2025/04/10/lawmakers-fear-ai-data-centers-will-drive-up-residents-power-bills/ Colorado Enacts Law Regulating High-Risk Artificial Intelligence ... https://www.americanbar .org/groups/business_law/resources/business-law-today/2024-july/colorado-enacts-law-regulating-high- risk-artificial-intelligence-systems/ Artificial Intelligence (AI) Legislation - multistate.ai https://www.multistate.ai/artificial-intelligence-ai-legislation [PDF] [FINAL] State AI Legislation Report - The Future of Privacy Forum https://fpf.org/wp-content/uploads/2024/09/FINAL-State-AI-Legislation-Report-webpage.pdf AI is set to drive surging electricity demand from data centres ... - IEA https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to- transform-how-the-energy-sector-works As Use of A.I. Soars, So Does the Energy and Water It Requires https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions CT HB05076 - BillTrack50 https://www.billtrack50.com/billdetail/1772898 Artificial Intelligence 2023 Legislation https://www.ncsl.org/technology-and-communication/artificial-intelligence-2023-legislation Study: How districts are responding to AI and what it means for the ... https://crpe.org/study-how-districts-are-responding-to-ai-and-what-it-means-for-the-new-school-year/ Board of Regents discuss AI's impact on education - New York State ... https://www.nyssba.org/news/2024/03/15/on-board-online-march-18-2024/board-of-regents-discuss-ai-s-impact-on-education/ Mayor Adams Releases First-of-Its-Kind Plan For Responsible Artificial Intelligence Use In NYC Gover | City of New York https://www.nyc.gov/office-of-the-mayor/news/777-23/mayor-adams-releases-first-of-its-kind-plan-responsible-artificial- intelligence-use-nyc Guidance for City staff using generative AI tools | SF.gov https://www.sf.gov/information--guidance-city-staff-using-generative-ai-tools Cantrell partnered with tech guru to launch AI-powered 311 chatbot ... https://veritenews.org/2025/03/28/cantrell-partnered-tech-guru-matt-wisdom-311/ From Policy to Practice: Pennsylvania's Pilot with ChatGPT in ... https://innovate-us.org/from-policy-to-practice-pennsylvania-s-pilot-with-chat-gpt-in-government Controversial AI Program Generates Photorealistic Police Sketches https://petapixel.com/2023/02/13/controversial-ai-program-generates-photorealistic-police-sketches/ Developers Created AI to Generate Police Sketches. Experts ... - VICE https://www.vice.com/en/article/ai-police-sketches/33 34 35 41 42 43 98 99106 37 38 50 53 54 55 56 57 58 59 62 65 40 47107 48 51 52 60 61 72 78 79 80 81 83 84 85 86 87 88 89 90 91 92 93 94 95 13 Los Angeles police accidentally release photos of undercover ... https://www.latimes.com/california/story/2023-03-21/los-angeles-police-accidentally-release-photos-of-undercover-officers-to- watchdog-website105 14